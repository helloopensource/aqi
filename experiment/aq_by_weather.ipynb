{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting charset-normalizer<4,>=2\n",
      "  Using cached charset_normalizer-3.4.2-cp39-cp39-macosx_10_9_universal2.whl (201 kB)\n",
      "Collecting idna<4,>=2.5\n",
      "  Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Installing collected packages: idna, charset-normalizer, certifi, requests\n",
      "Successfully installed certifi-2025.4.26 charset-normalizer-3.4.2 idna-3.10 requests-2.32.3\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.1.1 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting ipywidgets\n",
      "  Downloading ipywidgets-8.1.7-py3-none-any.whl (139 kB)\n",
      "Collecting jupyterlab_widgets~=3.0.15\n",
      "  Downloading jupyterlab_widgets-3.0.15-py3-none-any.whl (216 kB)\n",
      "Collecting widgetsnbextension~=4.0.14\n",
      "  Downloading widgetsnbextension-4.0.14-py3-none-any.whl (2.2 MB)\n",
      "Installing collected packages: widgetsnbextension, jupyterlab-widgets, ipywidgets\n",
      "Successfully installed ipywidgets-8.1.7 jupyterlab-widgets-3.0.15 widgetsnbextension-4.0.14\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.1.1 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.6.1-cp39-cp39-macosx_12_0_arm64.whl (11.1 MB)\n",
      "Collecting scipy>=1.6.0\n",
      "  Using cached scipy-1.13.1-cp39-cp39-macosx_12_0_arm64.whl (30.3 MB)\n",
      "Collecting joblib>=1.2.0\n",
      "  Using cached joblib-1.5.0-py3-none-any.whl (307 kB)\n",
      "Collecting threadpoolctl>=3.1.0\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.5.0 scikit-learn-1.6.1 scipy-1.13.1 threadpoolctl-3.6.0\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.1.1 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting autogluon\n",
      "  Using cached autogluon-1.3.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting autogluon.multimodal==1.3.0\n",
      "  Using cached autogluon.multimodal-1.3.0-py3-none-any.whl (454 kB)\n",
      "Collecting autogluon.tabular[all]==1.3.0\n",
      "  Using cached autogluon.tabular-1.3.0-py3-none-any.whl (382 kB)\n",
      "Collecting autogluon.timeseries[all]==1.3.0\n",
      "  Using cached autogluon.timeseries-1.3.0-py3-none-any.whl (181 kB)\n",
      "Collecting autogluon.core[all]==1.3.0\n",
      "  Using cached autogluon.core-1.3.0-py3-none-any.whl (222 kB)\n",
      "Collecting autogluon.features==1.3.0\n",
      "  Using cached autogluon.features-1.3.0-py3-none-any.whl (64 kB)\n",
      "Collecting matplotlib<3.11,>=3.7.0\n",
      "  Using cached matplotlib-3.9.4-cp39-cp39-macosx_11_0_arm64.whl (7.8 MB)\n",
      "Collecting networkx<4,>=3.0\n",
      "  Using cached networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
      "Collecting tqdm<5,>=4.38\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Collecting autogluon.common==1.3.0\n",
      "  Using cached autogluon.common-1.3.0-py3-none-any.whl (69 kB)\n",
      "Collecting ray[default,tune]<2.45,>=2.10.0\n",
      "  Using cached ray-2.44.1-cp39-cp39-macosx_11_0_arm64.whl (65.5 MB)\n",
      "Collecting pyarrow>=15.0.0\n",
      "  Using cached pyarrow-20.0.0-cp39-cp39-macosx_12_0_arm64.whl (30.8 MB)\n",
      "Collecting hyperopt<0.2.8,>=0.2.7\n",
      "  Using cached hyperopt-0.2.7-py2.py3-none-any.whl (1.6 MB)\n",
      "Collecting defusedxml<0.7.2,>=0.7.1\n",
      "  Using cached defusedxml-0.7.1-py2.py3-none-any.whl (25 kB)\n",
      "Collecting Pillow<12,>=10.0.1\n",
      "  Using cached pillow-11.2.1-cp39-cp39-macosx_11_0_arm64.whl (3.0 MB)\n",
      "Collecting pdf2image<1.19,>=1.17.0\n",
      "  Using cached pdf2image-1.17.0-py3-none-any.whl (11 kB)\n",
      "Collecting evaluate<0.5.0,>=0.4.0\n",
      "  Using cached evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
      "Collecting torchvision<0.22.0,>=0.16.0\n",
      "  Using cached torchvision-0.21.0-cp39-cp39-macosx_11_0_arm64.whl (1.8 MB)\n",
      "Collecting pytorch-metric-learning<2.9,>=1.3.0\n",
      "  Using cached pytorch_metric_learning-2.8.1-py3-none-any.whl (125 kB)\n",
      "Collecting jinja2<3.2,>=3.0.3\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Collecting timm<1.0.7,>=0.9.5\n",
      "  Using cached timm-1.0.3-py3-none-any.whl (2.3 MB)\n",
      "Collecting pytesseract<0.4,>=0.3.9\n",
      "  Using cached pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
      "Collecting lightning<2.7,>=2.2\n",
      "  Using cached lightning-2.5.1.post0-py3-none-any.whl (819 kB)\n",
      "Collecting nvidia-ml-py3<8.0,>=7.352.0\n",
      "  Using cached nvidia-ml-py3-7.352.0.tar.gz (19 kB)\n",
      "Collecting accelerate<2.0,>=0.34.0\n",
      "  Using cached accelerate-1.7.0-py3-none-any.whl (362 kB)\n",
      "Collecting torchmetrics<1.8,>=1.2.0\n",
      "  Using cached torchmetrics-1.7.1-py3-none-any.whl (961 kB)\n",
      "Collecting nltk<4.0,>=3.4.5\n",
      "  Using cached nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "Collecting transformers[sentencepiece]<4.50,>=4.38.0\n",
      "  Using cached transformers-4.49.0-py3-none-any.whl (10.0 MB)\n",
      "Collecting seqeval<1.3.0,>=1.2.2\n",
      "  Using cached seqeval-1.2.2.tar.gz (43 kB)\n",
      "Collecting tensorboard<3,>=2.9\n",
      "  Using cached tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\n",
      "Collecting text-unidecode<1.4,>=1.3\n",
      "  Using cached text_unidecode-1.3-py2.py3-none-any.whl (78 kB)\n",
      "Collecting nlpaug<1.2.0,>=1.1.10\n",
      "  Using cached nlpaug-1.1.11-py3-none-any.whl (410 kB)\n",
      "Collecting scikit-image<0.26.0,>=0.19.1\n",
      "  Using cached scikit_image-0.24.0-cp39-cp39-macosx_12_0_arm64.whl (13.4 MB)\n",
      "Collecting torch<2.7,>=2.2\n",
      "  Using cached torch-2.6.0-cp39-none-macosx_11_0_arm64.whl (66.5 MB)\n",
      "Collecting omegaconf<2.4.0,>=2.1.1\n",
      "  Using cached omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
      "Collecting openmim<0.4.0,>=0.3.7\n",
      "  Using cached openmim-0.3.9-py2.py3-none-any.whl (52 kB)\n",
      "Collecting jsonschema<4.24,>=4.18\n",
      "  Using cached jsonschema-4.23.0-py3-none-any.whl (88 kB)\n",
      "Collecting catboost<1.3,>=1.2\n",
      "  Using cached catboost-1.2.8-cp39-cp39-macosx_11_0_universal2.whl (27.8 MB)\n",
      "Collecting lightgbm<4.7,>=4.0\n",
      "  Using cached lightgbm-4.6.0-py3-none-macosx_12_0_arm64.whl (1.6 MB)\n",
      "Collecting xgboost<3.1,>=2.0\n",
      "  Using cached xgboost-2.1.4-py3-none-macosx_12_0_arm64.whl (1.9 MB)\n",
      "Collecting spacy<3.9\n",
      "  Using cached spacy-3.8.3-cp39-cp39-macosx_11_0_arm64.whl (6.3 MB)\n",
      "Collecting huggingface-hub[torch]\n",
      "  Using cached huggingface_hub-0.31.2-py3-none-any.whl (484 kB)\n",
      "Collecting fastai<2.9,>=2.3.1\n",
      "  Using cached fastai-2.7.19-py3-none-any.whl (234 kB)\n",
      "Collecting einops<0.9,>=0.7\n",
      "  Using cached einops-0.8.1-py3-none-any.whl (64 kB)\n",
      "Collecting utilsforecast<0.2.11,>=0.2.3\n",
      "  Using cached utilsforecast-0.2.10-py3-none-any.whl (41 kB)\n",
      "Collecting coreforecast<0.0.16,>=0.0.12\n",
      "  Using cached coreforecast-0.0.15-cp39-cp39-macosx_11_0_arm64.whl (194 kB)\n",
      "Collecting statsforecast<2.0.2,>=1.7.0\n",
      "  Using cached statsforecast-2.0.1-cp39-cp39-macosx_11_0_arm64.whl (298 kB)\n",
      "Collecting orjson~=3.9\n",
      "  Using cached orjson-3.10.18-cp39-cp39-macosx_10_15_x86_64.macosx_11_0_arm64.macosx_10_15_universal2.whl (249 kB)\n",
      "Collecting mlforecast<0.14,>0.13\n",
      "  Using cached mlforecast-0.13.6-py3-none-any.whl (71 kB)\n",
      "Collecting pytorch-lightning\n",
      "  Using cached pytorch_lightning-2.5.1.post0-py3-none-any.whl (823 kB)\n",
      "Collecting fugue>=0.9.0\n",
      "  Using cached fugue-0.9.1-py3-none-any.whl (278 kB)\n",
      "Collecting gluonts<0.17,>=0.15.0\n",
      "  Using cached gluonts-0.16.1-py3-none-any.whl (1.5 MB)\n",
      "Collecting pyyaml\n",
      "  Using cached PyYAML-6.0.2-cp39-cp39-macosx_11_0_arm64.whl (172 kB)\n",
      "Collecting safetensors>=0.4.3\n",
      "  Using cached safetensors-0.5.3-cp38-abi3-macosx_11_0_arm64.whl (418 kB)\n",
      "Collecting plotly\n",
      "  Using cached plotly-6.1.0-py3-none-any.whl (16.1 MB)\n",
      "Collecting graphviz\n",
      "  Using cached graphviz-0.20.3-py3-none-any.whl (47 kB)\n",
      "Collecting datasets>=2.0.0\n",
      "  Using cached datasets-3.6.0-py3-none-any.whl (491 kB)\n",
      "Collecting xxhash\n",
      "  Using cached xxhash-3.5.0-cp39-cp39-macosx_11_0_arm64.whl (30 kB)\n",
      "Collecting fsspec[http]>=2021.05.0\n",
      "  Using cached fsspec-2025.3.2-py3-none-any.whl (194 kB)\n",
      "Collecting multiprocess\n",
      "  Using cached multiprocess-0.70.18-py39-none-any.whl (133 kB)\n",
      "Collecting dill\n",
      "  Using cached dill-0.4.0-py3-none-any.whl (119 kB)\n",
      "Collecting filelock\n",
      "  Using cached filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Collecting fsspec[http]>=2021.05.0\n",
      "  Using cached fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "Collecting dill\n",
      "  Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Collecting multiprocess\n",
      "  Using cached multiprocess-0.70.16-py39-none-any.whl (133 kB)\n",
      "Collecting fastprogress>=0.2.4\n",
      "  Using cached fastprogress-1.0.3-py3-none-any.whl (12 kB)\n",
      "Collecting fastdownload<2,>=0.0.5\n",
      "  Using cached fastdownload-0.0.7-py3-none-any.whl (12 kB)\n",
      "Collecting fastcore<1.8,>=1.5.29\n",
      "  Using cached fastcore-1.7.29-py3-none-any.whl (84 kB)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1\n",
      "  Using cached aiohttp-3.11.18-cp39-cp39-macosx_11_0_arm64.whl (457 kB)\n",
      "Collecting attrs>=17.3.0\n",
      "  Using cached attrs-25.3.0-py3-none-any.whl (63 kB)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Using cached multidict-6.4.3-cp39-cp39-macosx_11_0_arm64.whl (37 kB)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Using cached frozenlist-1.6.0-cp39-cp39-macosx_11_0_arm64.whl (122 kB)\n",
      "Collecting async-timeout<6.0,>=4.0\n",
      "  Using cached async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Using cached aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Collecting yarl<2.0,>=1.17.0\n",
      "  Using cached yarl-1.20.0-cp39-cp39-macosx_11_0_arm64.whl (95 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0\n",
      "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Collecting propcache>=0.2.0\n",
      "  Using cached propcache-0.3.1-cp39-cp39-macosx_11_0_arm64.whl (46 kB)\n",
      "Collecting triad>=0.9.7\n",
      "  Using cached triad-0.9.8-py3-none-any.whl (62 kB)\n",
      "Collecting adagio>=0.2.4\n",
      "  Using cached adagio-0.2.6-py3-none-any.whl (19 kB)\n",
      "Collecting pydantic<3,>=1.7\n",
      "  Using cached pydantic-2.11.4-py3-none-any.whl (443 kB)\n",
      "Collecting toolz~=0.10\n",
      "  Using cached toolz-0.12.1-py3-none-any.whl (56 kB)\n",
      "Collecting py4j\n",
      "  Using cached py4j-0.10.9.9-py2.py3-none-any.whl (203 kB)\n",
      "Collecting cloudpickle\n",
      "  Using cached cloudpickle-3.1.1-py3-none-any.whl (20 kB)\n",
      "Collecting MarkupSafe>=2.0\n",
      "  Using cached MarkupSafe-3.0.2-cp39-cp39-macosx_11_0_arm64.whl (12 kB)\n",
      "Collecting rpds-py>=0.7.1\n",
      "  Using cached rpds_py-0.25.0-cp39-cp39-macosx_11_0_arm64.whl (359 kB)\n",
      "Collecting referencing>=0.28.4\n",
      "  Using cached referencing-0.36.2-py3-none-any.whl (26 kB)\n",
      "Collecting jsonschema-specifications>=2023.03.6\n",
      "  Using cached jsonschema_specifications-2025.4.1-py3-none-any.whl (18 kB)\n",
      "Collecting packaging>=20.0\n",
      "  Using cached packaging-24.2-py3-none-any.whl (65 kB)\n",
      "Collecting lightning-utilities<2.0,>=0.10.0\n",
      "  Using cached lightning_utilities-0.14.3-py3-none-any.whl (28 kB)\n",
      "Collecting kiwisolver>=1.3.1\n",
      "  Using cached kiwisolver-1.4.7-cp39-cp39-macosx_11_0_arm64.whl (64 kB)\n",
      "Collecting contourpy>=1.0.1\n",
      "  Using cached contourpy-1.3.0-cp39-cp39-macosx_11_0_arm64.whl (249 kB)\n",
      "Collecting cycler>=0.10\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Collecting importlib-resources>=3.2.0\n",
      "  Using cached importlib_resources-6.5.2-py3-none-any.whl (37 kB)\n",
      "Collecting fonttools>=4.22.0\n",
      "  Using cached fonttools-4.58.0-cp39-cp39-macosx_10_9_universal2.whl (2.7 MB)\n",
      "Collecting pyparsing>=2.3.1\n",
      "  Using cached pyparsing-3.2.3-py3-none-any.whl (111 kB)\n",
      "Collecting numba\n",
      "  Using cached numba-0.60.0-cp39-cp39-macosx_11_0_arm64.whl (2.7 MB)\n",
      "Collecting optuna\n",
      "  Using cached optuna-4.3.0-py3-none-any.whl (386 kB)\n",
      "Collecting window-ops\n",
      "  Using cached window_ops-0.0.15-py3-none-any.whl (15 kB)\n",
      "Collecting gdown>=4.0.0\n",
      "  Using cached gdown-5.2.0-py3-none-any.whl (18 kB)\n",
      "Collecting beautifulsoup4\n",
      "  Using cached beautifulsoup4-4.13.4-py3-none-any.whl (187 kB)\n",
      "Collecting regex>=2021.8.3\n",
      "  Using cached regex-2024.11.6-cp39-cp39-macosx_11_0_arm64.whl (284 kB)\n",
      "Collecting click\n",
      "  Using cached click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Collecting antlr4-python3-runtime==4.9.*\n",
      "  Using cached antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
      "Collecting rich\n",
      "  Using cached rich-14.0.0-py3-none-any.whl (243 kB)\n",
      "Collecting tabulate\n",
      "  Using cached tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Collecting model-index\n",
      "  Using cached model_index-0.1.11-py3-none-any.whl (34 kB)\n",
      "Collecting opendatalab\n",
      "  Using cached opendatalab-0.0.10-py3-none-any.whl (29 kB)\n",
      "Collecting colorama\n",
      "  Using cached colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Collecting pydantic-core==2.33.2\n",
      "  Using cached pydantic_core-2.33.2-cp39-cp39-macosx_11_0_arm64.whl (1.9 MB)\n",
      "Collecting annotated-types>=0.6.0\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Collecting typing-inspection>=0.4.0\n",
      "  Using cached typing_inspection-0.4.0-py3-none-any.whl (14 kB)\n",
      "Collecting msgpack<2.0.0,>=1.0.0\n",
      "  Using cached msgpack-1.1.0-cp39-cp39-macosx_11_0_arm64.whl (81 kB)\n",
      "Collecting protobuf!=3.19.5,>=3.15.3\n",
      "  Using cached protobuf-6.31.0-cp39-abi3-macosx_10_9_universal2.whl (425 kB)\n",
      "Collecting tensorboardX>=1.9\n",
      "  Using cached tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
      "Collecting aiohttp-cors\n",
      "  Using cached aiohttp_cors-0.8.1-py3-none-any.whl (25 kB)\n",
      "Collecting prometheus-client>=0.7.1\n",
      "  Using cached prometheus_client-0.22.0-py3-none-any.whl (62 kB)\n",
      "Collecting opencensus\n",
      "  Using cached opencensus-0.11.4-py2.py3-none-any.whl (128 kB)\n",
      "Collecting py-spy>=0.2.0\n",
      "  Using cached py_spy-0.4.0-py2.py3-none-macosx_11_0_arm64.whl (1.8 MB)\n",
      "Collecting colorful\n",
      "  Using cached colorful-0.5.6-py2.py3-none-any.whl (201 kB)\n",
      "Collecting virtualenv!=20.21.1,>=20.0.24\n",
      "  Using cached virtualenv-20.31.2-py3-none-any.whl (6.1 MB)\n",
      "Collecting smart-open\n",
      "  Using cached smart_open-7.1.0-py3-none-any.whl (61 kB)\n",
      "Collecting grpcio>=1.32.0\n",
      "  Using cached grpcio-1.71.0-cp39-cp39-macosx_10_14_universal2.whl (11.3 MB)\n",
      "Collecting lazy-loader>=0.4\n",
      "  Using cached lazy_loader-0.4-py3-none-any.whl (12 kB)\n",
      "Collecting tifffile>=2022.8.12\n",
      "  Using cached tifffile-2024.8.30-py3-none-any.whl (227 kB)\n",
      "Collecting imageio>=2.33\n",
      "  Using cached imageio-2.37.0-py3-none-any.whl (315 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Using cached spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3\n",
      "  Using cached srsly-2.5.1-cp39-cp39-macosx_11_0_arm64.whl (635 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Using cached murmurhash-1.0.12-cp39-cp39-macosx_11_0_arm64.whl (26 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6\n",
      "  Using cached catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Using cached cymem-2.0.11-cp39-cp39-macosx_11_0_arm64.whl (42 kB)\n",
      "Collecting thinc<8.4.0,>=8.3.0\n",
      "  Using cached thinc-8.3.6-cp39-cp39-macosx_11_0_arm64.whl (848 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11\n",
      "  Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Collecting typer<1.0.0,>=0.3.0\n",
      "  Using cached typer-0.15.4-py3-none-any.whl (45 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Using cached preshed-3.0.9-cp39-cp39-macosx_11_0_arm64.whl (129 kB)\n",
      "Collecting weasel<0.5.0,>=0.1.0\n",
      "  Using cached weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "Collecting langcodes<4.0.0,>=3.2.0\n",
      "  Using cached langcodes-3.5.0-py3-none-any.whl (182 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1\n",
      "  Using cached wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Collecting language-data>=1.2\n",
      "  Using cached language_data-1.3.0-py3-none-any.whl (5.4 MB)\n",
      "Collecting marisa-trie>=1.1.0\n",
      "  Using cached marisa_trie-1.2.1-cp39-cp39-macosx_11_0_arm64.whl (175 kB)\n",
      "Collecting statsmodels>=0.13.2\n",
      "  Using cached statsmodels-0.14.4-cp39-cp39-macosx_11_0_arm64.whl (9.9 MB)\n",
      "Collecting llvmlite<0.44,>=0.43.0dev0\n",
      "  Using cached llvmlite-0.43.0-cp39-cp39-macosx_11_0_arm64.whl (28.8 MB)\n",
      "Collecting patsy>=0.5.6\n",
      "  Using cached patsy-1.0.1-py2.py3-none-any.whl (232 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Using cached markdown-3.8-py3-none-any.whl (106 kB)\n",
      "Collecting werkzeug>=1.0.1\n",
      "  Using cached werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "Collecting absl-py>=0.4\n",
      "  Using cached absl_py-2.2.2-py3-none-any.whl (135 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0\n",
      "  Using cached tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Collecting blis<1.4.0,>=1.3.0\n",
      "  Using cached blis-1.3.0-cp39-cp39-macosx_15_0_arm64.whl\n",
      "Collecting confection<1.0.0,>=0.0.1\n",
      "  Using cached confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Collecting sympy==1.13.1\n",
      "  Using cached sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "Collecting mpmath<1.4,>=1.1.0\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Collecting tokenizers<0.22,>=0.21\n",
      "  Using cached tokenizers-0.21.1-cp39-abi3-macosx_11_0_arm64.whl (2.7 MB)\n",
      "Collecting sentencepiece!=0.1.92,>=0.1.91\n",
      "  Using cached sentencepiece-0.2.0-cp39-cp39-macosx_11_0_arm64.whl (1.2 MB)\n",
      "Collecting fs\n",
      "  Using cached fs-2.4.16-py2.py3-none-any.whl (135 kB)\n",
      "Collecting shellingham>=1.3.0\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Collecting markdown-it-py>=2.2.0\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Collecting mdurl~=0.1\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Collecting distlib<1,>=0.3.7\n",
      "  Using cached distlib-0.3.9-py2.py3-none-any.whl (468 kB)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0\n",
      "  Using cached cloudpathlib-0.21.1-py3-none-any.whl (52 kB)\n",
      "Collecting wrapt\n",
      "  Using cached wrapt-1.17.2-cp39-cp39-macosx_11_0_arm64.whl (38 kB)\n",
      "Collecting soupsieve>1.2\n",
      "  Using cached soupsieve-2.7-py3-none-any.whl (36 kB)\n",
      "Collecting appdirs~=1.4.3\n",
      "  Using cached appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Collecting ordered-set\n",
      "  Using cached ordered_set-4.1.0-py3-none-any.whl (7.6 kB)\n",
      "Collecting opencensus-context>=0.1.3\n",
      "  Using cached opencensus_context-0.1.3-py2.py3-none-any.whl (5.1 kB)\n",
      "Collecting google-api-core<3.0.0,>=1.0.0\n",
      "  Using cached google_api_core-2.24.2-py3-none-any.whl (160 kB)\n",
      "Collecting six\n",
      "  Using cached six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
      "Collecting googleapis-common-protos<2.0.0,>=1.56.2\n",
      "  Using cached googleapis_common_protos-1.70.0-py3-none-any.whl (294 kB)\n",
      "Collecting google-auth<3.0.0,>=2.14.1\n",
      "  Using cached google_auth-2.40.1-py2.py3-none-any.whl (216 kB)\n",
      "Collecting proto-plus<2.0.0,>=1.22.3\n",
      "  Using cached proto_plus-1.26.1-py3-none-any.whl (50 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Using cached rsa-4.9.1-py3-none-any.whl (34 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Using cached cachetools-5.5.2-py3-none-any.whl (10 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Using cached pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
      "Collecting pyasn1<0.7.0,>=0.6.1\n",
      "  Using cached pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "Collecting openxlab\n",
      "  Using cached openxlab-0.1.2-py3-none-any.whl (311 kB)\n",
      "Collecting pycryptodome\n",
      "  Downloading pycryptodome-3.23.0-cp37-abi3-macosx_10_9_universal2.whl (2.5 MB)\n",
      "Collecting openxlab\n",
      "  Using cached openxlab-0.1.1-py3-none-any.whl (308 kB)\n",
      "  Using cached openxlab-0.1.0-py3-none-any.whl (307 kB)\n",
      "  Using cached openxlab-0.0.38-py3-none-any.whl (302 kB)\n",
      "  Using cached openxlab-0.0.37-py3-none-any.whl (302 kB)\n",
      "  Using cached openxlab-0.0.36-py3-none-any.whl (302 kB)\n",
      "  Using cached openxlab-0.0.35-py3-none-any.whl (302 kB)\n",
      "  Using cached openxlab-0.0.34-py3-none-any.whl (299 kB)\n",
      "  Using cached openxlab-0.0.33-py3-none-any.whl (299 kB)\n",
      "  Using cached openxlab-0.0.32-py3-none-any.whl (298 kB)\n",
      "  Using cached openxlab-0.0.31-py3-none-any.whl (298 kB)\n",
      "  Using cached openxlab-0.0.30-py3-none-any.whl (298 kB)\n",
      "  Using cached openxlab-0.0.29-py3-none-any.whl (297 kB)\n",
      "  Using cached openxlab-0.0.28-py3-none-any.whl (297 kB)\n",
      "  Using cached openxlab-0.0.27-py3-none-any.whl (296 kB)\n",
      "  Using cached openxlab-0.0.26-py3-none-any.whl (295 kB)\n",
      "  Using cached openxlab-0.0.25-py3-none-any.whl (295 kB)\n",
      "  Using cached openxlab-0.0.24-py3-none-any.whl (291 kB)\n",
      "  Using cached openxlab-0.0.23-py3-none-any.whl (299 kB)\n",
      "  Using cached openxlab-0.0.22-py3-none-any.whl (300 kB)\n",
      "  Using cached openxlab-0.0.21-py3-none-any.whl (300 kB)\n",
      "  Using cached openxlab-0.0.20-py3-none-any.whl (300 kB)\n",
      "  Using cached openxlab-0.0.19-py3-none-any.whl (300 kB)\n",
      "  Using cached openxlab-0.0.18-py3-none-any.whl (300 kB)\n",
      "  Using cached openxlab-0.0.17-py3-none-any.whl (291 kB)\n",
      "  Using cached openxlab-0.0.16-py3-none-any.whl (289 kB)\n",
      "  Using cached openxlab-0.0.15-py3-none-any.whl (289 kB)\n",
      "  Using cached openxlab-0.0.14-py3-none-any.whl (288 kB)\n",
      "  Using cached openxlab-0.0.13-py3-none-any.whl (282 kB)\n",
      "  Using cached openxlab-0.0.12-py3-none-any.whl (54 kB)\n",
      "  Using cached openxlab-0.0.11-py3-none-any.whl (55 kB)\n",
      "Collecting colorlog\n",
      "  Using cached colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
      "Collecting alembic>=1.5.0\n",
      "  Using cached alembic-1.15.2-py3-none-any.whl (231 kB)\n",
      "Collecting sqlalchemy>=1.4.2\n",
      "  Using cached sqlalchemy-2.0.41-cp39-cp39-macosx_11_0_arm64.whl (2.1 MB)\n",
      "Collecting Mako\n",
      "  Using cached mako-1.3.10-py3-none-any.whl (78 kB)\n",
      "Collecting narwhals>=1.15.1\n",
      "  Using cached narwhals-1.39.1-py3-none-any.whl (355 kB)\n",
      "Collecting PySocks!=1.5.7,>=1.5.6\n",
      "  Using cached PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Building wheels for collected packages: nvidia-ml-py3, antlr4-python3-runtime, seqeval\n",
      "  Building wheel for nvidia-ml-py3 (setup.py): started\n",
      "  Building wheel for nvidia-ml-py3 (setup.py): finished with status 'done'\n",
      "  Created wheel for nvidia-ml-py3: filename=nvidia_ml_py3-7.352.0-py3-none-any.whl size=19191 sha256=22e3842dd4027a0a623ab6a8f136fb31bdee9453815269a67e1d2ab92df33b2a\n",
      "  Stored in directory: /Users/changliang.wu/Library/Caches/pip/wheels/f6/d8/b0/15cfd7805d39250ac29318105f09b1750683387630d68423e1\n",
      "  Building wheel for antlr4-python3-runtime (setup.py): started\n",
      "  Building wheel for antlr4-python3-runtime (setup.py): finished with status 'done'\n",
      "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144575 sha256=3b8aea24000a35065d75cefa549b098dd662ed8fc0c0ae4a2feae0181a4c3244\n",
      "  Stored in directory: /Users/changliang.wu/Library/Caches/pip/wheels/23/cf/80/f3efa822e6ab23277902ee9165fe772eeb1dfb8014f359020a\n",
      "  Building wheel for seqeval (setup.py): started\n",
      "  Building wheel for seqeval (setup.py): finished with status 'done'\n",
      "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16181 sha256=5e12a3ba2c0a36aae2d48ad9d69cda977c29f33bf7e707e9a3423b7f7869935f\n",
      "  Stored in directory: /Users/changliang.wu/Library/Caches/pip/wheels/e2/a5/92/2c80d1928733611c2747a9820e1324a6835524d9411510c142\n",
      "Successfully built nvidia-ml-py3 antlr4-python3-runtime seqeval\n",
      "Installing collected packages: six, rpds-py, pyasn1, attrs, rsa, referencing, pyasn1-modules, protobuf, propcache, multidict, mpmath, mdurl, MarkupSafe, frozenlist, cachetools, appdirs, yarl, typing-inspection, tqdm, sympy, pyyaml, pyparsing, pydantic-core, pyarrow, proto-plus, Pillow, packaging, networkx, markdown-it-py, kiwisolver, jsonschema-specifications, jinja2, importlib-resources, googleapis-common-protos, google-auth, fsspec, fs, fonttools, filelock, cycler, contourpy, catalogue, async-timeout, annotated-types, aiosignal, aiohappyeyeballs, wrapt, triad, torch, srsly, sqlalchemy, shellingham, rich, pydantic, opencensus-context, narwhals, murmurhash, msgpack, matplotlib, marisa-trie, Mako, llvmlite, lightning-utilities, jsonschema, huggingface-hub, google-api-core, distlib, cymem, click, autogluon.common, aiohttp, wasabi, virtualenv, typer, torchmetrics, tokenizers, tensorboardX, soupsieve, smart-open, safetensors, regex, ray, PySocks, py4j, py-spy, prometheus-client, preshed, plotly, patsy, opencensus, numba, language-data, grpcio, graphviz, dill, confection, colorlog, colorful, cloudpickle, cloudpathlib, blis, autogluon.features, autogluon.core, alembic, aiohttp-cors, adagio, xxhash, xgboost, window-ops, werkzeug, weasel, utilsforecast, transformers, toolz, thinc, tensorboard-data-server, statsmodels, spacy-loggers, spacy-legacy, sentencepiece, pytorch-lightning, pycryptodome, ordered-set, optuna, openxlab, multiprocess, markdown, lightgbm, langcodes, hyperopt, fugue, fastprogress, fastcore, coreforecast, colorama, catboost, beautifulsoup4, autogluon.tabular, absl-py, torchvision, tifffile, tensorboard, tabulate, statsforecast, spacy, orjson, opendatalab, model-index, mlforecast, lightning, lazy-loader, imageio, gluonts, gdown, fastdownload, datasets, antlr4-python3-runtime, accelerate, timm, text-unidecode, seqeval, scikit-image, pytorch-metric-learning, pytesseract, pdf2image, openmim, omegaconf, nvidia-ml-py3, nltk, nlpaug, fastai, evaluate, einops, defusedxml, autogluon.timeseries, autogluon.multimodal, autogluon\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 25.0\n",
      "    Uninstalling packaging-25.0:\n",
      "      Successfully uninstalled packaging-25.0\n",
      "Successfully installed Mako-1.3.10 MarkupSafe-3.0.2 Pillow-11.2.1 PySocks-1.7.1 absl-py-2.2.2 accelerate-1.7.0 adagio-0.2.6 aiohappyeyeballs-2.6.1 aiohttp-3.11.18 aiohttp-cors-0.8.1 aiosignal-1.3.2 alembic-1.15.2 annotated-types-0.7.0 antlr4-python3-runtime-4.9.3 appdirs-1.4.4 async-timeout-5.0.1 attrs-25.3.0 autogluon-1.3.0 autogluon.common-1.3.0 autogluon.core-1.3.0 autogluon.features-1.3.0 autogluon.multimodal-1.3.0 autogluon.tabular-1.3.0 autogluon.timeseries-1.3.0 beautifulsoup4-4.13.4 blis-1.3.0 cachetools-5.5.2 catalogue-2.0.10 catboost-1.2.8 click-8.1.8 cloudpathlib-0.21.1 cloudpickle-3.1.1 colorama-0.4.6 colorful-0.5.6 colorlog-6.9.0 confection-0.1.5 contourpy-1.3.0 coreforecast-0.0.15 cycler-0.12.1 cymem-2.0.11 datasets-3.6.0 defusedxml-0.7.1 dill-0.3.8 distlib-0.3.9 einops-0.8.1 evaluate-0.4.3 fastai-2.7.19 fastcore-1.7.29 fastdownload-0.0.7 fastprogress-1.0.3 filelock-3.18.0 fonttools-4.58.0 frozenlist-1.6.0 fs-2.4.16 fsspec-2025.3.0 fugue-0.9.1 gdown-5.2.0 gluonts-0.16.1 google-api-core-2.24.2 google-auth-2.40.1 googleapis-common-protos-1.70.0 graphviz-0.20.3 grpcio-1.71.0 huggingface-hub-0.31.2 hyperopt-0.2.7 imageio-2.37.0 importlib-resources-6.5.2 jinja2-3.1.6 jsonschema-4.23.0 jsonschema-specifications-2025.4.1 kiwisolver-1.4.7 langcodes-3.5.0 language-data-1.3.0 lazy-loader-0.4 lightgbm-4.6.0 lightning-2.5.1.post0 lightning-utilities-0.14.3 llvmlite-0.43.0 marisa-trie-1.2.1 markdown-3.8 markdown-it-py-3.0.0 matplotlib-3.9.4 mdurl-0.1.2 mlforecast-0.13.6 model-index-0.1.11 mpmath-1.3.0 msgpack-1.1.0 multidict-6.4.3 multiprocess-0.70.16 murmurhash-1.0.12 narwhals-1.39.1 networkx-3.2.1 nlpaug-1.1.11 nltk-3.9.1 numba-0.60.0 nvidia-ml-py3-7.352.0 omegaconf-2.3.0 opencensus-0.11.4 opencensus-context-0.1.3 opendatalab-0.0.10 openmim-0.3.9 openxlab-0.0.11 optuna-4.3.0 ordered-set-4.1.0 orjson-3.10.18 packaging-24.2 patsy-1.0.1 pdf2image-1.17.0 plotly-6.1.0 preshed-3.0.9 prometheus-client-0.22.0 propcache-0.3.1 proto-plus-1.26.1 protobuf-6.31.0 py-spy-0.4.0 py4j-0.10.9.9 pyarrow-20.0.0 pyasn1-0.6.1 pyasn1-modules-0.4.2 pycryptodome-3.23.0 pydantic-2.11.4 pydantic-core-2.33.2 pyparsing-3.2.3 pytesseract-0.3.13 pytorch-lightning-2.5.1.post0 pytorch-metric-learning-2.8.1 pyyaml-6.0.2 ray-2.44.1 referencing-0.36.2 regex-2024.11.6 rich-14.0.0 rpds-py-0.25.0 rsa-4.9.1 safetensors-0.5.3 scikit-image-0.24.0 sentencepiece-0.2.0 seqeval-1.2.2 shellingham-1.5.4 six-1.17.0 smart-open-7.1.0 soupsieve-2.7 spacy-3.8.3 spacy-legacy-3.0.12 spacy-loggers-1.0.5 sqlalchemy-2.0.41 srsly-2.5.1 statsforecast-2.0.1 statsmodels-0.14.4 sympy-1.13.1 tabulate-0.9.0 tensorboard-2.19.0 tensorboard-data-server-0.7.2 tensorboardX-2.6.2.2 text-unidecode-1.3 thinc-8.3.6 tifffile-2024.8.30 timm-1.0.3 tokenizers-0.21.1 toolz-0.12.1 torch-2.6.0 torchmetrics-1.7.1 torchvision-0.21.0 tqdm-4.67.1 transformers-4.49.0 triad-0.9.8 typer-0.15.4 typing-inspection-0.4.0 utilsforecast-0.2.10 virtualenv-20.31.2 wasabi-1.1.3 weasel-0.4.1 werkzeug-3.1.3 window-ops-0.0.15 wrapt-1.17.2 xgboost-2.1.4 xxhash-3.5.0 yarl-1.20.0\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.1.1 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.1.1 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting nbconvert\n",
      "  Downloading nbconvert-7.16.6-py3-none-any.whl (258 kB)\n",
      "Collecting nbclient>=0.5.0\n",
      "  Downloading nbclient-0.10.2-py3-none-any.whl (25 kB)\n",
      "Collecting mistune<4,>=2.0.3\n",
      "  Downloading mistune-3.1.3-py3-none-any.whl (53 kB)\n",
      "Collecting bleach[css]!=5.0.0\n",
      "  Downloading bleach-6.2.0-py3-none-any.whl (163 kB)\n",
      "Collecting nbformat>=5.7\n",
      "  Downloading nbformat-5.10.4-py3-none-any.whl (78 kB)\n",
      "Collecting jupyterlab-pygments\n",
      "  Downloading jupyterlab_pygments-0.3.0-py3-none-any.whl (15 kB)\n",
      "Collecting pandocfilters>=1.4.1\n",
      "  Downloading pandocfilters-1.5.1-py2.py3-none-any.whl (8.7 kB)\n",
      "Collecting webencodings\n",
      "  Downloading webencodings-0.5.1-py2.py3-none-any.whl (11 kB)\n",
      "Collecting tinycss2<1.5,>=1.1.0\n",
      "  Downloading tinycss2-1.4.0-py3-none-any.whl (26 kB)\n",
      "Collecting fastjsonschema>=2.15\n",
      "  Downloading fastjsonschema-2.21.1-py3-none-any.whl (23 kB)\n",
      "Installing collected packages: webencodings, fastjsonschema, tinycss2, nbformat, bleach, pandocfilters, nbclient, mistune, jupyterlab-pygments, nbconvert\n",
      "Successfully installed bleach-6.2.0 fastjsonschema-2.21.1 jupyterlab-pygments-0.3.0 mistune-3.1.3 nbclient-0.10.2 nbconvert-7.16.6 nbformat-5.10.4 pandocfilters-1.5.1 tinycss2-1.4.0 webencodings-0.5.1\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.1.1 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting python-dotenv\n",
      "  Using cached python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
      "Installing collected packages: python-dotenv\n",
      "Successfully installed python-dotenv-1.1.0\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.1.1 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Set up your environment according to the repo's environment.yml file or run the following...\n",
    "# Comment these out, once installed or otherwise not needed.\n",
    "# This creates an empty pip_requirements.txt file used to suppress 'already satisfied' output.\n",
    "import os\n",
    "with open('pip_requirements.txt', mode='a'): pass\n",
    "%pip install boto3        -r pip_requirements.txt | grep -v 'already satisfied'\n",
    "%pip install pandas       -r pip_requirements.txt | grep -v 'already satisfied'\n",
    "%pip install numpy        -r pip_requirements.txt | grep -v 'already satisfied'\n",
    "%pip install requests     -r pip_requirements.txt | grep -v 'already satisfied'\n",
    "%pip install ipywidgets   -r pip_requirements.txt | grep -v 'already satisfied'\n",
    "%pip install scikit-learn -r pip_requirements.txt | grep -v 'already satisfied'\n",
    "%pip install autogluon    -r pip_requirements.txt | grep -v 'already satisfied'\n",
    "%pip install matplotlib   -r pip_requirements.txt | grep -v 'already satisfied'\n",
    "%pip install nbconvert    -r pip_requirements.txt | grep -v 'already satisfied'\n",
    "%pip install python-dotenv    -r pip_requirements.txt | grep -v 'already satisfied'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statements for packages used...\n",
    "import os, glob, shutil, sys, requests, json\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "\n",
    "from botocore import UNSIGNED\n",
    "from botocore.config import Config\n",
    "from io import StringIO\n",
    "from datetime import datetime\n",
    "from types import SimpleNamespace\n",
    "from IPython.display import clear_output\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()  # This loads variables from .env into os.environ\n",
    "\n",
    "# Now you can access them\n",
    "api_key = os.getenv('OPENAQ_API_KEY')\n",
    "\n",
    "# The following is required for matplotlib plots to display in some envs...\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes and Variables are ready.\n"
     ]
    }
   ],
   "source": [
    "# class AQParam => Used to define attributes for the (6) main OpenAQ parameters.\n",
    "class AQParam:\n",
    "    def __init__(self, id, name, unit, unhealthyThresholdDefault, desc):\n",
    "        self.id                        = id\n",
    "        self.name                      = name\n",
    "        self.unit                      = unit\n",
    "        self.unhealthyThresholdDefault = unhealthyThresholdDefault\n",
    "        self.desc                      = desc\n",
    "    \n",
    "    def isValid(self):\n",
    "        if(self is not None and self.id > 0 and self.unhealthyThresholdDefault > 0.0 and \n",
    "           len(self.name) > 0 and len(self.unit) > 0 and len(self.desc) > 0):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "            \n",
    "    def toJSON(self):\n",
    "        return json.dumps(self, default=lambda o: o.__dict__, sort_keys=True, indent=2)\n",
    "\n",
    "# class AQScenario => Defines an ML scenario including a Location w/ NOAA Weather Station ID \n",
    "#                     and the target OpenAQ Param.\n",
    "# Note: OpenAQ data mostly begins sometime in 2016, so using that as a default yearStart value.\n",
    "class AQScenario:\n",
    "    def __init__(self, location=None, noaaStationID=None, aqParamTarget=None, unhealthyThreshold=None, \n",
    "                 yearStart=2016, yearEnd=2024, aqRadiusMiles=10, featureColumnsToDrop=None):\n",
    "        self.location           = location\n",
    "        self.name               = location + \"_\" + aqParamTarget.name\n",
    "        self.noaaStationID      = noaaStationID\n",
    "        self.noaaStationLat     = 0.0\n",
    "        self.noaaStationLng     = 0.0\n",
    "        self.openAqSensorIDs       = []\n",
    "        \n",
    "        self.aqParamTarget      = aqParamTarget\n",
    "        \n",
    "        if unhealthyThreshold and unhealthyThreshold > 0.0:\n",
    "            self.unhealthyThreshold = unhealthyThreshold\n",
    "        else:\n",
    "            self.unhealthyThreshold = self.aqParamTarget.unhealthyThresholdDefault\n",
    "        \n",
    "        self.yearStart          = yearStart\n",
    "        self.yearEnd            = yearEnd\n",
    "        self.aqRadiusMiles      = aqRadiusMiles\n",
    "        self.aqRadiusMeters     = aqRadiusMiles * 1610 # Rough integer approximation is fine here.\n",
    "        \n",
    "        self.modelFolder        = \"AutogluonModels\"\n",
    "            \n",
    "    def getSummary(self):\n",
    "        return f\"Scenario: {self.name} => {self.aqParamTarget.desc} ({self.aqParamTarget.name}) with UnhealthyThreshold > {self.unhealthyThreshold} {self.aqParamTarget.unit}\"\n",
    "    \n",
    "    def getModelPath(self):\n",
    "        return f\"{self.modelFolder}/aq_{self.name}_{self.yearStart}-{self.yearEnd}/\"\n",
    "    \n",
    "    def updateNoaaStationLatLng(self, noaagsod_df_row):\n",
    "        # Use a NOAA row to set Lat+Lng values used for the OpenAQ API requests...\n",
    "        if(noaagsod_df_row is not None and noaagsod_df_row['LATITUDE'] and noaagsod_df_row['LONGITUDE']):\n",
    "            self.noaaStationLat = noaagsod_df_row['LATITUDE']\n",
    "            self.noaaStationLng = noaagsod_df_row['LONGITUDE']\n",
    "            print(f\"NOAA Station Lat,Lng Updated for Scenario: {self.name} => {self.noaaStationLat},{self.noaaStationLng}\")\n",
    "        else:\n",
    "            print(\"NOAA Station Lat,Lng COULD NOT BE UPDATED.\")\n",
    "    \n",
    "    def isValid(self):\n",
    "        if(self is not None and self.aqParamTarget is not None and\n",
    "           self.yearStart > 0 and self.yearEnd > 0 and self.yearEnd >= self.yearStart and \n",
    "           self.aqRadiusMiles > 0 and self.aqRadiusMeters > 0 and self.unhealthyThreshold > 0.0 and \n",
    "           len(self.name) > 0 and len(self.noaaStationID) > 0):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "            \n",
    "    def toJSON(self):\n",
    "        return json.dumps(self, default=lambda o: o.__dict__, sort_keys=True, indent=2)\n",
    "\n",
    "# class AQbyWeatherApp => Main app class with settings, AQParams, AQScenarios, and data access methods...\n",
    "class AQbyWeatherApp:\n",
    "    def __init__(self, mlTargetLabel='isUnhealthy', mlEvalMetric='accuracy', mlTimeLimitSecs=None):\n",
    "        self.mlTargetLabel   = mlTargetLabel\n",
    "        self.mlEvalMetric    = mlEvalMetric\n",
    "        self.mlTimeLimitSecs = mlTimeLimitSecs\n",
    "        self.mlIgnoreColumns = ['DATE','NAME','LATITUDE','LONGITUDE','day','avg']\n",
    "        \n",
    "        self.defaultColumnsNOAA   = ['DATE','NAME','LATITUDE','LONGITUDE',\n",
    "                                     'DEWP','WDSP','MAX','MIN','PRCP','MONTH'] # Default relevant NOAA columns\n",
    "        # self.defaultColumnsOpenAQ = ['summary']       # Default relevant OpenAQ columns\n",
    "        \n",
    "        self.aqParams    = {} # A list to save AQParam objects\n",
    "        self.aqScenarios = {} # A list to save AQScenario objects\n",
    "        \n",
    "        self.selectedScenario = None\n",
    "    \n",
    "    def addAQParam(self, aqParam):\n",
    "        if aqParam and aqParam.isValid():\n",
    "            self.aqParams[aqParam.name] = aqParam\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def addAQScenario(self, aqScenario):\n",
    "        if aqScenario and aqScenario.isValid():\n",
    "            self.aqScenarios[aqScenario.name] = aqScenario\n",
    "            if(self.selectedScenario is None):\n",
    "                self.selectedScenario = self.aqScenarios[next(iter(self.aqScenarios))] # Default selectedScenario to 1st item.\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def getFilenameNOAA(self):\n",
    "        if self and self.selectedScenario and self.selectedScenario.isValid():\n",
    "            return f\"dataNOAA_{self.selectedScenario.name}_{self.selectedScenario.yearStart}-{self.selectedScenario.yearEnd}_{self.selectedScenario.noaaStationID}.csv\"\n",
    "        else:\n",
    "            return \"\"\n",
    "    \n",
    "    def getFilenameOpenAQ(self):\n",
    "        if self and self.selectedScenario and self.selectedScenario.isValid() and len(self.selectedScenario.openAqSensorIDs) > 0:\n",
    "            idString = \"\"\n",
    "            for i in range(0, len(self.selectedScenario.openAqSensorIDs)):\n",
    "                idString = idString + str(self.selectedScenario.openAqSensorIDs[i]) + \"-\"\n",
    "            idString = idString[:-1]\n",
    "            return f\"dataOpenAQ_{self.selectedScenario.name}_{self.selectedScenario.yearStart}-{self.selectedScenario.yearEnd}_{idString}.csv\"\n",
    "        else:\n",
    "            return \"\"\n",
    "    \n",
    "    def getFilenameOther(self, prefix):\n",
    "        if self and self.selectedScenario and self.selectedScenario.isValid():\n",
    "            return f\"{prefix}_{self.selectedScenario.name}_{self.selectedScenario.yearStart}-{self.selectedScenario.yearEnd}.csv\"\n",
    "    \n",
    "    def getNoaaDataFrame(self):\n",
    "        # ASDI Dataset Name: NOAA GSOD\n",
    "        # ASDI Dataset URL : https://registry.opendata.aws/noaa-gsod/\n",
    "        # NOAA GSOD README : https://www.ncei.noaa.gov/data/global-summary-of-the-day/doc/readme.txt\n",
    "        # NOAA GSOD data in S3 is organized by year and Station ID values, so this is straight-forward\n",
    "        # Example S3 path format => s3://noaa-gsod-pds/{yyyy}/{stationid}.csv\n",
    "        # Let's start with a new DataFrame and load it from a local CSV or the NOAA data source...\n",
    "        noaagsod_df = pd.DataFrame()\n",
    "        filenameNOAA = self.getFilenameNOAA()\n",
    "\n",
    "        if os.path.exists(filenameNOAA):\n",
    "            # Use local data file already accessed + prepared...\n",
    "            print('Loading NOAA GSOD data from local file: ', filenameNOAA)\n",
    "            noaagsod_df = pd.read_csv(filenameNOAA)\n",
    "        else:\n",
    "            # Access + prepare data and save to a local data file...\n",
    "            noaagsod_bucket = 'noaa-gsod-pds'\n",
    "            print(f'Accessing and preparing data from ASDI-hosted NOAA GSOD dataset in Amazon S3 (bucket: {noaagsod_bucket})...')\n",
    "            s3 = boto3.client('s3', config=Config(signature_version=UNSIGNED))\n",
    "\n",
    "            for year in range(self.selectedScenario.yearStart, self.selectedScenario.yearEnd + 1):\n",
    "                key = f'{year}/{self.selectedScenario.noaaStationID}.csv'                                    # Compute the key to get\n",
    "                csv_obj = s3.get_object(Bucket=noaagsod_bucket, Key=key)                                     # Get the S3 object\n",
    "                csv_string = csv_obj['Body'].read().decode('utf-8')                                          # Read object contents to a string\n",
    "                noaagsod_df = pd.concat([noaagsod_df, pd.read_csv(StringIO(csv_string))], ignore_index=True) # Use the string to build the DataFrame\n",
    "\n",
    "            # It may be true that Month affects air quality (ie: seasonal considerations; tends to have correlation for certain areas)\n",
    "            # Extract date components for seasonality\n",
    "            noaagsod_df['MONTH'] = pd.to_datetime(noaagsod_df['DATE']).dt.month\n",
    "            noaagsod_df['DAYOFWEEK'] = pd.to_datetime(noaagsod_df['DATE']).dt.dayofweek\n",
    "            noaagsod_df['SEASON'] = pd.to_datetime(noaagsod_df['DATE']).dt.month.map({1: 'Winter', 2: 'Winter', 3: 'Spring', \n",
    "                                                                                      4: 'Spring', 5: 'Spring', 6: 'Summer',\n",
    "                                                                                      7: 'Summer', 8: 'Summer', 9: 'Fall', \n",
    "                                                                                      10: 'Fall', 11: 'Fall', 12: 'Winter'})\n",
    "            \n",
    "            # Calculate temperature differences and averages\n",
    "            noaagsod_df['TEMP_RANGE'] = noaagsod_df['MAX'] - noaagsod_df['MIN']\n",
    "            noaagsod_df['TEMP_AVG'] = (noaagsod_df['MAX'] + noaagsod_df['MIN']) / 2\n",
    "            \n",
    "            # Create interaction features\n",
    "            noaagsod_df['TEMP_DEWP_DIFF'] = noaagsod_df['TEMP_AVG'] - noaagsod_df['DEWP']\n",
    "            noaagsod_df['WDSP_TEMP'] = noaagsod_df['WDSP'] * noaagsod_df['TEMP_AVG']\n",
    "\n",
    "            # Trim down to the desired key columns... (do this last in case engineered columns are to be removed)\n",
    "            # noaagsod_df = noaagsod_df[self.defaultColumnsNOAA]\n",
    "            \n",
    "        return noaagsod_df\n",
    "        \n",
    "    def getOpenAqDataFrame(self):\n",
    "        # ASDI Dataset Name: OpenAQ\n",
    "        # ASDI Dataset URL : https://registry.opendata.aws/openaq/\n",
    "        # OpenAQ API Docs  : https://docs.openaq.org/\n",
    "        # OpenAQ S3 data is only organized by date folders, so each folder is large and contains all stations.\n",
    "        # Because of this, it's better to query ASDI OpenAQ data using the CloudFront-hosted API.\n",
    "        # Note that some days may not have values and will get filtered out via an INNER JOIN later.\n",
    "        # Let's start with a new DataFrame and load it from a local CSV or the NOAA data source...\n",
    "        aq_df = pd.DataFrame()\n",
    "        aq_reqUrlBase = \"https://api.openaq.org/v3\" # OpenAQ ASDI API Endpoint URL Base\n",
    "        print(f\"API Key: {api_key}\")\n",
    "        headers = {\n",
    "            'accept': 'application/json',\n",
    "            'x-api-key': api_key\n",
    "        }\n",
    "\n",
    "        if self.selectedScenario.noaaStationLat == 0.0 or self.selectedScenario.noaaStationLng == 0.0:\n",
    "            print(\"NOAA Station Lat/Lng NOT DEFINED. CANNOT PROCEED\")\n",
    "            return aq_df\n",
    "        \n",
    "        if len(self.selectedScenario.openAqSensorIDs) == 0:\n",
    "            # Find OpenAQ sensors near the NOAA station location\n",
    "            print('Finding OpenAQ sensors near NOAA station location...')\n",
    "            \n",
    "            # Query OpenAQ locations API with coordinates\n",
    "            aq_reqParams = {\n",
    "                'coordinates': f\"{self.selectedScenario.noaaStationLat},{self.selectedScenario.noaaStationLng}\",\n",
    "                'radius': 25000, # 25km radius\n",
    "                'parameter': self.selectedScenario.aqParamTarget.name,\n",
    "                'limit': 100\n",
    "            }\n",
    "            \n",
    "            aq_resp = requests.get(aq_reqUrlBase + \"/locations\", params=aq_reqParams, headers=headers)\n",
    "            aq_data = aq_resp.json()\n",
    "            \n",
    "            if 'results' in aq_data:\n",
    "                for location in aq_data['results']:\n",
    "                    # Check each location's sensors for our target parameter\n",
    "                    for sensor in location['sensors']:\n",
    "                        if sensor['parameter']['name'] == self.selectedScenario.aqParamTarget.name:\n",
    "                            self.selectedScenario.openAqSensorIDs.append(sensor['id'])\n",
    "                            break # Only need one sensor per location\n",
    "                            \n",
    "            print(f'Found {len(self.selectedScenario.openAqSensorIDs)} OpenAQ locations with {self.selectedScenario.aqParamTarget.name} sensors')\n",
    "        \n",
    "        if len(self.selectedScenario.openAqSensorIDs) >= 1:\n",
    "            filenameOpenAQ = self.getFilenameOpenAQ()\n",
    "\n",
    "            if os.path.exists(filenameOpenAQ):\n",
    "                # Use local data file already accessed + prepared...\n",
    "                print('Loading OpenAQ data from local file: ', filenameOpenAQ)\n",
    "                aq_df = pd.read_csv(filenameOpenAQ)\n",
    "            else:\n",
    "                # Access + prepare data (NOTE: calling OpenAQ API one year at a time to avoid timeouts)\n",
    "                print('Accessing ASDI-hosted OpenAQ Measurements (HTTPS API)...')\n",
    "                \n",
    "                for year in range(self.selectedScenario.yearStart, self.selectedScenario.yearEnd + 1):\n",
    "                    for sensor_id in self.selectedScenario.openAqSensorIDs:\n",
    "                        # Get daily measurements for this sensor and year\n",
    "                        aq_reqUrl = f\"{aq_reqUrlBase}/sensors/{sensor_id}/days\"\n",
    "                        aq_reqParams = {\n",
    "                            'date_from': f'{year}-01-01',\n",
    "                            'date_to': f'{year}-12-31',\n",
    "                            'limit': 366\n",
    "                        }\n",
    "                        \n",
    "                        print(f'Fetching data for sensor {sensor_id} in {year}')\n",
    "                        aq_resp = requests.get(aq_reqUrl, params=aq_reqParams, headers=headers)\n",
    "                        aq_data = aq_resp.json()\n",
    "                        \n",
    "                        if 'results' in aq_data:\n",
    "                            for measurement in aq_data['results']:\n",
    "                                dt = datetime.strptime(measurement['period']['datetimeFrom']['utc'], '%Y-%m-%dT%H:%M:%SZ')\n",
    "                                if measurement['value'] is not None:\n",
    "                                    date_df = pd.DataFrame({'day': [dt.date()], 'avg': [measurement['value']]})\n",
    "                                    aq_df = pd.concat([aq_df, date_df], ignore_index=True)\n",
    "\n",
    "                # Group by day and calculate daily averages\n",
    "                if not aq_df.empty:\n",
    "                    aq_df = aq_df.groupby('day')['avg'].mean().reset_index()\n",
    "\n",
    "                # Perform some Label Engineering to add our binary classification label => {0=OKAY, 1=UNHEALTHY}\n",
    "                if not aq_df.empty:\n",
    "                    aq_df[self.mlTargetLabel] = np.where(aq_df['avg'] <= self.selectedScenario.unhealthyThreshold, 0, 1)\n",
    "        \n",
    "        return aq_df\n",
    "    \n",
    "    def getMergedDataFrame(self, noaagsod_df, aq_df):\n",
    "        if len(noaagsod_df) > 0 and len(aq_df) > 0:\n",
    "            # Print shapes before merge to debug\n",
    "            print(f\"NOAA GSOD shape before merge: {noaagsod_df.shape}\")\n",
    "            print(f\"AQ data shape before merge: {aq_df.shape}\")\n",
    "            print(\"\\nNOAA GSOD sample:\")\n",
    "            print(noaagsod_df.head())\n",
    "            print(\"\\nAQ data sample:\")\n",
    "            print(aq_df.head())\n",
    "            \n",
    "            merged_df = pd.merge(noaagsod_df, aq_df, how=\"inner\", left_on=\"DATE\", right_on=\"day\")\n",
    "            \n",
    "            # Print shape after merge to see if rows were lost\n",
    "            print(f\"\\nMerged shape: {merged_df.shape}\")\n",
    "            \n",
    "            if len(merged_df) == 0:\n",
    "                print(\"\\nMerge resulted in empty DataFrame. This means there are no matching dates between the two datasets.\")\n",
    "                print(\"Check that DATE and day columns have the same format (both should be datetime or string)\")\n",
    "                print(f\"DATE dtype: {noaagsod_df['DATE'].dtype}\")\n",
    "                print(f\"day dtype: {aq_df['day'].dtype}\")\n",
    "            \n",
    "            display(merged_df)\n",
    "            merged_df = merged_df.drop(columns=self.mlIgnoreColumns)\n",
    "            return merged_df\n",
    "        else:\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def getConfusionMatrixData(self, cm):\n",
    "        cmData = SimpleNamespace()\n",
    "        cmData.TN = cm[0][0]\n",
    "        cmData.TP = cm[1][1]\n",
    "        cmData.FN = cm[1][0]\n",
    "        cmData.FP = cm[0][1]\n",
    "        \n",
    "        cmData.TN_Rate = cmData.TN/(cmData.TN+cmData.FP)\n",
    "        cmData.TP_Rate = cmData.TP/(cmData.TP+cmData.FN)\n",
    "        cmData.FN_Rate = cmData.FN/(cmData.FN+cmData.TP)\n",
    "        cmData.FP_Rate = cmData.FP/(cmData.FP+cmData.TN)\n",
    "        \n",
    "        cmData.TN_Output = f\"True Negatives  (TN): {cmData.TN} of {cmData.TN+cmData.FP} => {round(cmData.TN_Rate * 100, 2)}%\"\n",
    "        cmData.TP_Output = f\"True Positives  (TP): {cmData.TP} of {cmData.TP+cmData.FN} => {round(cmData.TP_Rate * 100, 2)}%\"\n",
    "        cmData.FN_Output = f\"False Negatives (FN): {cmData.FN} of {cmData.FN+cmData.TP} => {round(cmData.FN_Rate * 100, 2)}%\"\n",
    "        cmData.FP_Output = f\"False Positives (FP): {cmData.FP} of {cmData.FP+cmData.TN} => {round(cmData.FP_Rate * 100, 2)}%\"\n",
    "        \n",
    "        return cmData\n",
    "            \n",
    "print(\"Classes and Variables are ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AQbyWeather.aqParams: 6\n",
      "AQbyWeather.aqScenarios: 15 (Default Selected: bakersfield_pm25)\n"
     ]
    }
   ],
   "source": [
    "# CELL #4: Review the pre-defined AQParams and AQScenarios in this cell. You can edit these and/or use your own...\n",
    "# AQParams are added with default thresholds, which can be overridden on a per-AQScenario basis.\n",
    "# These AQParams are based on the OpenAQ /parameters API call where isCore=true (https://api.openaq.org/v2/parameters).\n",
    "# Default thresholds where provided using data from EPA.gov (https://www.epa.gov/criteria-air-pollutants/naaqs-table).\n",
    "# Confirm and adjust params or thresholds as needed for your needs... Not for scientific or health purposes.\n",
    "\n",
    "# Instantiate main App class with explicit mlTargetLabel and mlEvalMetric provided...\n",
    "AQbyWeather = AQbyWeatherApp(mlTargetLabel='isUnhealthy', mlEvalMetric='accuracy', mlTimeLimitSecs=120)\n",
    "\n",
    "# Define and add new AQParams...\n",
    "AQbyWeather.addAQParam(AQParam( 1, \"pm10\", \"µg/m³\", 150.0, \"Particulate Matter < 10 micrometers\"))\n",
    "AQbyWeather.addAQParam(AQParam( 2, \"pm25\", \"µg/m³\",  12.0, \"Particulate Matter < 2.5 micrometers\"))\n",
    "AQbyWeather.addAQParam(AQParam( 7, \"no2\",  \"ppm\",   100.0, \"Nitrogen Dioxide\"))\n",
    "AQbyWeather.addAQParam(AQParam( 8, \"co\",   \"ppm\",     9.0, \"Carbon Monoxide\"))\n",
    "AQbyWeather.addAQParam(AQParam( 9, \"so2\",  \"ppm\",    75.0, \"Sulfur Dioxide\"))\n",
    "AQbyWeather.addAQParam(AQParam(10, \"o3\",   \"ppm\",   0.070, \"Ground Level Ozone\"))\n",
    "\n",
    "# Define available AQ Scenarios for certain locations with their associated NOAA GSOD StationID values...\n",
    "# NOAA GSOD Station Search: https://www.ncei.noaa.gov/access/search/data-search/global-summary-of-the-day\n",
    "# TODO: Someday consider how to OPTIONALLY append more scenarios via an optional JSON file\n",
    "#       (ie: without adding a dependecy outside the .ipynb file)\n",
    "# NOTE: For Ozone Scenarios, we're generally using 0.035 ppm to override the default threshold.\n",
    "AQbyWeather.addAQScenario(AQScenario(\"bakersfield\", \"72384023155\", AQbyWeather.aqParams[\"pm25\"], None))\n",
    "AQbyWeather.addAQScenario(AQScenario(\"bakersfield\", \"72384023155\", AQbyWeather.aqParams[\"pm10\"], None)) # Attempt at pm10 prediction.\n",
    "AQbyWeather.addAQScenario(AQScenario(\"bakersfield\", \"72384023155\", AQbyWeather.aqParams[\"o3\"],  0.035))\n",
    "AQbyWeather.addAQScenario(AQScenario(\"fresno\",      \"72389093193\", AQbyWeather.aqParams[\"pm25\"], None))\n",
    "AQbyWeather.addAQScenario(AQScenario(\"fresno\",      \"72389093193\", AQbyWeather.aqParams[\"o3\"],  0.035))\n",
    "AQbyWeather.addAQScenario(AQScenario(\"visalia\",     \"72389693144\", AQbyWeather.aqParams[\"pm25\"], None))\n",
    "AQbyWeather.addAQScenario(AQScenario(\"visalia\",     \"72389693144\", AQbyWeather.aqParams[\"o3\"],  0.035))\n",
    "AQbyWeather.addAQScenario(AQScenario(\"san-jose\",    \"72494693232\", AQbyWeather.aqParams[\"pm25\"], None))\n",
    "AQbyWeather.addAQScenario(AQScenario(\"san-jose\",    \"72494693232\", AQbyWeather.aqParams[\"o3\"],  0.035))\n",
    "AQbyWeather.addAQScenario(AQScenario(\"los-angeles\", \"72287493134\", AQbyWeather.aqParams[\"pm25\"], None))\n",
    "AQbyWeather.addAQScenario(AQScenario(\"los-angeles\", \"72287493134\", AQbyWeather.aqParams[\"o3\"],  0.035))\n",
    "AQbyWeather.addAQScenario(AQScenario(\"phoenix\",     \"72278023183\", AQbyWeather.aqParams[\"pm25\"], None))\n",
    "AQbyWeather.addAQScenario(AQScenario(\"phoenix\",     \"72278023183\", AQbyWeather.aqParams[\"o3\"],  0.035))\n",
    "AQbyWeather.addAQScenario(AQScenario(\"fairbanks\",   \"70261026411\", AQbyWeather.aqParams[\"pm25\"], None))\n",
    "AQbyWeather.addAQScenario(AQScenario(\"lahore-pk\",   \"41640099999\", AQbyWeather.aqParams[\"pm25\"], None))\n",
    "\n",
    "print(f\"AQbyWeather.aqParams: {str(len(AQbyWeather.aqParams))}\")\n",
    "print(f\"AQbyWeather.aqScenarios: {str(len(AQbyWeather.aqScenarios))} (Default Selected: {AQbyWeather.selectedScenario.name})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** CHOOSE YOUR OWN ADVENTURE HERE ***\n",
      "Please select a Scenario via the following drop-down-list...\n",
      "(NOTE: If you change Scenario, you must re-run remaining cells to see changes.)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db98410966704a9482d58abf31cc1299",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(index=9, options=('bakersfield_pm25', 'bakersfield_pm10', 'bakersfield_o3', 'fresno_pm25', 'fresno_o3…"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CELL #5: Select a Scenario via DROP DOWN LIST to use throughout the Notebook. This will drive the ML process...\n",
    "# A default \"value\" is set to avoid issues. Change this default to run the Notebook from start-to-finish for that Scenario.\n",
    "print(\"*** CHOOSE YOUR OWN ADVENTURE HERE ***\")\n",
    "print(\"Please select a Scenario via the following drop-down-list...\")\n",
    "print(\"(NOTE: If you change Scenario, you must re-run remaining cells to see changes.)\")\n",
    "ddl = widgets.Dropdown(options=AQbyWeather.aqScenarios.keys(), \n",
    "                       value=AQbyWeather.aqScenarios[\"los-angeles_pm25\"].name) # <-- DEFAULT / FULL-RUN VALUE\n",
    "ddl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scenario: los-angeles_pm25 => Particulate Matter < 2.5 micrometers (pm25) with UnhealthyThreshold > 12.0 µg/m³\n",
      "{\n",
      "  \"aqParamTarget\": {\n",
      "    \"desc\": \"Particulate Matter < 2.5 micrometers\",\n",
      "    \"id\": 2,\n",
      "    \"name\": \"pm25\",\n",
      "    \"unhealthyThresholdDefault\": 12.0,\n",
      "    \"unit\": \"\\u00b5g/m\\u00b3\"\n",
      "  },\n",
      "  \"aqRadiusMeters\": 16100,\n",
      "  \"aqRadiusMiles\": 10,\n",
      "  \"location\": \"los-angeles\",\n",
      "  \"modelFolder\": \"AutogluonModels\",\n",
      "  \"name\": \"los-angeles_pm25\",\n",
      "  \"noaaStationID\": \"72287493134\",\n",
      "  \"noaaStationLat\": 0.0,\n",
      "  \"noaaStationLng\": 0.0,\n",
      "  \"openAqSensorIDs\": [],\n",
      "  \"unhealthyThreshold\": 12.0,\n",
      "  \"yearEnd\": 2024,\n",
      "  \"yearStart\": 2016\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "if ddl.value:\n",
    "    AQbyWeather.selectedScenario = AQbyWeather.aqScenarios[ddl.value]\n",
    "    print(AQbyWeather.selectedScenario.getSummary())\n",
    "    print(AQbyWeather.selectedScenario.toJSON())\n",
    "else:\n",
    "    print(\"Please select a Scenario via the above drop-down-list.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scenario: los-angeles_pm25 => Particulate Matter < 2.5 micrometers (pm25) with UnhealthyThreshold > 12.0 µg/m³\n",
      "Accessing and preparing data from ASDI-hosted NOAA GSOD dataset in Amazon S3 (bucket: noaa-gsod-pds)...\n",
      "NOAA Station Lat,Lng Updated for Scenario: los-angeles_pm25 => 34.0236,-118.2911\n",
      "noaagsod_df.shape = (3061, 35)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STATION</th>\n",
       "      <th>DATE</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>ELEVATION</th>\n",
       "      <th>NAME</th>\n",
       "      <th>TEMP</th>\n",
       "      <th>TEMP_ATTRIBUTES</th>\n",
       "      <th>DEWP</th>\n",
       "      <th>DEWP_ATTRIBUTES</th>\n",
       "      <th>...</th>\n",
       "      <th>PRCP_ATTRIBUTES</th>\n",
       "      <th>SNDP</th>\n",
       "      <th>FRSHTT</th>\n",
       "      <th>MONTH</th>\n",
       "      <th>DAYOFWEEK</th>\n",
       "      <th>SEASON</th>\n",
       "      <th>TEMP_RANGE</th>\n",
       "      <th>TEMP_AVG</th>\n",
       "      <th>TEMP_DEWP_DIFF</th>\n",
       "      <th>WDSP_TEMP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>72287493134</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>34.0236</td>\n",
       "      <td>-118.2911</td>\n",
       "      <td>54.6</td>\n",
       "      <td>LOS ANGELES DOWNTOWN USC, CA US</td>\n",
       "      <td>52.7</td>\n",
       "      <td>24</td>\n",
       "      <td>19.1</td>\n",
       "      <td>24</td>\n",
       "      <td>...</td>\n",
       "      <td>G</td>\n",
       "      <td>999.9</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>Winter</td>\n",
       "      <td>23.9</td>\n",
       "      <td>52.95</td>\n",
       "      <td>33.85</td>\n",
       "      <td>84.720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>72287493134</td>\n",
       "      <td>2016-01-02</td>\n",
       "      <td>34.0236</td>\n",
       "      <td>-118.2911</td>\n",
       "      <td>54.6</td>\n",
       "      <td>LOS ANGELES DOWNTOWN USC, CA US</td>\n",
       "      <td>54.8</td>\n",
       "      <td>24</td>\n",
       "      <td>20.1</td>\n",
       "      <td>24</td>\n",
       "      <td>...</td>\n",
       "      <td>G</td>\n",
       "      <td>999.9</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>Winter</td>\n",
       "      <td>21.9</td>\n",
       "      <td>53.95</td>\n",
       "      <td>33.85</td>\n",
       "      <td>113.295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>72287493134</td>\n",
       "      <td>2016-01-03</td>\n",
       "      <td>34.0236</td>\n",
       "      <td>-118.2911</td>\n",
       "      <td>54.6</td>\n",
       "      <td>LOS ANGELES DOWNTOWN USC, CA US</td>\n",
       "      <td>52.6</td>\n",
       "      <td>24</td>\n",
       "      <td>38.2</td>\n",
       "      <td>24</td>\n",
       "      <td>...</td>\n",
       "      <td>G</td>\n",
       "      <td>999.9</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>Winter</td>\n",
       "      <td>20.8</td>\n",
       "      <td>54.50</td>\n",
       "      <td>16.30</td>\n",
       "      <td>38.150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>72287493134</td>\n",
       "      <td>2016-01-04</td>\n",
       "      <td>34.0236</td>\n",
       "      <td>-118.2911</td>\n",
       "      <td>54.6</td>\n",
       "      <td>LOS ANGELES DOWNTOWN USC, CA US</td>\n",
       "      <td>59.3</td>\n",
       "      <td>24</td>\n",
       "      <td>44.1</td>\n",
       "      <td>24</td>\n",
       "      <td>...</td>\n",
       "      <td>G</td>\n",
       "      <td>999.9</td>\n",
       "      <td>10000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Winter</td>\n",
       "      <td>25.0</td>\n",
       "      <td>56.60</td>\n",
       "      <td>12.50</td>\n",
       "      <td>62.260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>72287493134</td>\n",
       "      <td>2016-01-05</td>\n",
       "      <td>34.0236</td>\n",
       "      <td>-118.2911</td>\n",
       "      <td>54.6</td>\n",
       "      <td>LOS ANGELES DOWNTOWN USC, CA US</td>\n",
       "      <td>57.4</td>\n",
       "      <td>24</td>\n",
       "      <td>51.9</td>\n",
       "      <td>24</td>\n",
       "      <td>...</td>\n",
       "      <td>G</td>\n",
       "      <td>999.9</td>\n",
       "      <td>110000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Winter</td>\n",
       "      <td>15.1</td>\n",
       "      <td>61.55</td>\n",
       "      <td>9.65</td>\n",
       "      <td>129.255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3056</th>\n",
       "      <td>72287493134</td>\n",
       "      <td>2024-05-16</td>\n",
       "      <td>34.0236</td>\n",
       "      <td>-118.2911</td>\n",
       "      <td>54.6</td>\n",
       "      <td>LOS ANGELES DOWNTOWN USC, CA US</td>\n",
       "      <td>62.9</td>\n",
       "      <td>24</td>\n",
       "      <td>53.2</td>\n",
       "      <td>24</td>\n",
       "      <td>...</td>\n",
       "      <td>G</td>\n",
       "      <td>999.9</td>\n",
       "      <td>10000</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>Spring</td>\n",
       "      <td>7.9</td>\n",
       "      <td>64.05</td>\n",
       "      <td>10.85</td>\n",
       "      <td>51.240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3057</th>\n",
       "      <td>72287493134</td>\n",
       "      <td>2024-05-17</td>\n",
       "      <td>34.0236</td>\n",
       "      <td>-118.2911</td>\n",
       "      <td>54.6</td>\n",
       "      <td>LOS ANGELES DOWNTOWN USC, CA US</td>\n",
       "      <td>63.2</td>\n",
       "      <td>24</td>\n",
       "      <td>53.8</td>\n",
       "      <td>24</td>\n",
       "      <td>...</td>\n",
       "      <td>G</td>\n",
       "      <td>999.9</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>Spring</td>\n",
       "      <td>9.0</td>\n",
       "      <td>64.60</td>\n",
       "      <td>10.80</td>\n",
       "      <td>58.140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3058</th>\n",
       "      <td>72287493134</td>\n",
       "      <td>2024-05-18</td>\n",
       "      <td>34.0236</td>\n",
       "      <td>-118.2911</td>\n",
       "      <td>54.6</td>\n",
       "      <td>LOS ANGELES DOWNTOWN USC, CA US</td>\n",
       "      <td>62.9</td>\n",
       "      <td>24</td>\n",
       "      <td>53.5</td>\n",
       "      <td>24</td>\n",
       "      <td>...</td>\n",
       "      <td>G</td>\n",
       "      <td>999.9</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>Spring</td>\n",
       "      <td>10.1</td>\n",
       "      <td>64.05</td>\n",
       "      <td>10.55</td>\n",
       "      <td>83.265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3059</th>\n",
       "      <td>72287493134</td>\n",
       "      <td>2024-05-19</td>\n",
       "      <td>34.0236</td>\n",
       "      <td>-118.2911</td>\n",
       "      <td>54.6</td>\n",
       "      <td>LOS ANGELES DOWNTOWN USC, CA US</td>\n",
       "      <td>62.9</td>\n",
       "      <td>24</td>\n",
       "      <td>52.1</td>\n",
       "      <td>24</td>\n",
       "      <td>...</td>\n",
       "      <td>G</td>\n",
       "      <td>999.9</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>Spring</td>\n",
       "      <td>13.0</td>\n",
       "      <td>65.50</td>\n",
       "      <td>13.40</td>\n",
       "      <td>91.700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3060</th>\n",
       "      <td>72287493134</td>\n",
       "      <td>2024-05-20</td>\n",
       "      <td>34.0236</td>\n",
       "      <td>-118.2911</td>\n",
       "      <td>54.6</td>\n",
       "      <td>LOS ANGELES DOWNTOWN USC, CA US</td>\n",
       "      <td>59.9</td>\n",
       "      <td>15</td>\n",
       "      <td>52.0</td>\n",
       "      <td>15</td>\n",
       "      <td>...</td>\n",
       "      <td>G</td>\n",
       "      <td>999.9</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>Spring</td>\n",
       "      <td>15.0</td>\n",
       "      <td>64.50</td>\n",
       "      <td>12.50</td>\n",
       "      <td>83.850</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3061 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          STATION        DATE  LATITUDE  LONGITUDE  ELEVATION  \\\n",
       "0     72287493134  2016-01-01   34.0236  -118.2911       54.6   \n",
       "1     72287493134  2016-01-02   34.0236  -118.2911       54.6   \n",
       "2     72287493134  2016-01-03   34.0236  -118.2911       54.6   \n",
       "3     72287493134  2016-01-04   34.0236  -118.2911       54.6   \n",
       "4     72287493134  2016-01-05   34.0236  -118.2911       54.6   \n",
       "...           ...         ...       ...        ...        ...   \n",
       "3056  72287493134  2024-05-16   34.0236  -118.2911       54.6   \n",
       "3057  72287493134  2024-05-17   34.0236  -118.2911       54.6   \n",
       "3058  72287493134  2024-05-18   34.0236  -118.2911       54.6   \n",
       "3059  72287493134  2024-05-19   34.0236  -118.2911       54.6   \n",
       "3060  72287493134  2024-05-20   34.0236  -118.2911       54.6   \n",
       "\n",
       "                                 NAME  TEMP  TEMP_ATTRIBUTES  DEWP  \\\n",
       "0     LOS ANGELES DOWNTOWN USC, CA US  52.7               24  19.1   \n",
       "1     LOS ANGELES DOWNTOWN USC, CA US  54.8               24  20.1   \n",
       "2     LOS ANGELES DOWNTOWN USC, CA US  52.6               24  38.2   \n",
       "3     LOS ANGELES DOWNTOWN USC, CA US  59.3               24  44.1   \n",
       "4     LOS ANGELES DOWNTOWN USC, CA US  57.4               24  51.9   \n",
       "...                               ...   ...              ...   ...   \n",
       "3056  LOS ANGELES DOWNTOWN USC, CA US  62.9               24  53.2   \n",
       "3057  LOS ANGELES DOWNTOWN USC, CA US  63.2               24  53.8   \n",
       "3058  LOS ANGELES DOWNTOWN USC, CA US  62.9               24  53.5   \n",
       "3059  LOS ANGELES DOWNTOWN USC, CA US  62.9               24  52.1   \n",
       "3060  LOS ANGELES DOWNTOWN USC, CA US  59.9               15  52.0   \n",
       "\n",
       "      DEWP_ATTRIBUTES  ...  PRCP_ATTRIBUTES   SNDP  FRSHTT  MONTH  DAYOFWEEK  \\\n",
       "0                  24  ...                G  999.9       0      1          4   \n",
       "1                  24  ...                G  999.9       0      1          5   \n",
       "2                  24  ...                G  999.9       0      1          6   \n",
       "3                  24  ...                G  999.9   10000      1          0   \n",
       "4                  24  ...                G  999.9  110000      1          1   \n",
       "...               ...  ...              ...    ...     ...    ...        ...   \n",
       "3056               24  ...                G  999.9   10000      5          3   \n",
       "3057               24  ...                G  999.9       0      5          4   \n",
       "3058               24  ...                G  999.9       0      5          5   \n",
       "3059               24  ...                G  999.9       0      5          6   \n",
       "3060               15  ...                G  999.9       0      5          0   \n",
       "\n",
       "      SEASON  TEMP_RANGE  TEMP_AVG  TEMP_DEWP_DIFF  WDSP_TEMP  \n",
       "0     Winter        23.9     52.95           33.85     84.720  \n",
       "1     Winter        21.9     53.95           33.85    113.295  \n",
       "2     Winter        20.8     54.50           16.30     38.150  \n",
       "3     Winter        25.0     56.60           12.50     62.260  \n",
       "4     Winter        15.1     61.55            9.65    129.255  \n",
       "...      ...         ...       ...             ...        ...  \n",
       "3056  Spring         7.9     64.05           10.85     51.240  \n",
       "3057  Spring         9.0     64.60           10.80     58.140  \n",
       "3058  Spring        10.1     64.05           10.55     83.265  \n",
       "3059  Spring        13.0     65.50           13.40     91.700  \n",
       "3060  Spring        15.0     64.50           12.50     83.850  \n",
       "\n",
       "[3061 rows x 35 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# GET NOAA GSOD WEATHER DATA...\n",
    "print(AQbyWeather.selectedScenario.getSummary())\n",
    "noaagsod_df = AQbyWeather.getNoaaDataFrame()\n",
    "\n",
    "if(len(noaagsod_df) >= 1):\n",
    "    # Update NOAA Station Lat/Lng...\n",
    "    AQbyWeather.selectedScenario.updateNoaaStationLatLng(noaagsod_df.iloc[0])\n",
    "    \n",
    "    # Save DataFrame to CSV...\n",
    "    noaagsod_df.to_csv(AQbyWeather.getFilenameNOAA(), index=False)\n",
    "\n",
    "    # Output DataFrame properties...\n",
    "    print('noaagsod_df.shape =', noaagsod_df.shape)\n",
    "    display(noaagsod_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scenario: los-angeles_pm25 => Particulate Matter < 2.5 micrometers (pm25) with UnhealthyThreshold > 12.0 µg/m³\n",
      "API Key: 49c492bb9d9e24d5e78ae75b2034430080bc132bcd7c9dfa3f2b08fabda35a9d\n",
      "Finding OpenAQ sensors near NOAA station location...\n",
      "Found 90 OpenAQ locations with pm25 sensors\n",
      "Accessing ASDI-hosted OpenAQ Measurements (HTTPS API)...\n",
      "Fetching data for sensor 2775 in 2016\n",
      "Fetching data for sensor 25551 in 2016\n",
      "Fetching data for sensor 15731 in 2016\n",
      "Fetching data for sensor 25196 in 2016\n",
      "Fetching data for sensor 24000 in 2016\n",
      "Fetching data for sensor 1654141 in 2016\n",
      "Fetching data for sensor 1654143 in 2016\n",
      "Fetching data for sensor 1654156 in 2016\n",
      "Fetching data for sensor 1654168 in 2016\n",
      "Fetching data for sensor 1654191 in 2016\n",
      "Fetching data for sensor 2000869 in 2016\n",
      "Fetching data for sensor 1654217 in 2016\n",
      "Fetching data for sensor 2000834 in 2016\n",
      "Fetching data for sensor 1654333 in 2016\n",
      "Fetching data for sensor 1654360 in 2016\n",
      "Fetching data for sensor 2000475 in 2016\n",
      "Fetching data for sensor 2000731 in 2016\n",
      "Fetching data for sensor 2000674 in 2016\n",
      "Fetching data for sensor 1654499 in 2016\n",
      "Fetching data for sensor 1654545 in 2016\n",
      "Fetching data for sensor 1654577 in 2016\n",
      "Fetching data for sensor 1654556 in 2016\n",
      "Fetching data for sensor 1654629 in 2016\n",
      "Fetching data for sensor 2000900 in 2016\n",
      "Fetching data for sensor 1999896 in 2016\n",
      "Fetching data for sensor 2001073 in 2016\n",
      "Fetching data for sensor 2001289 in 2016\n",
      "Fetching data for sensor 2001343 in 2016\n",
      "Fetching data for sensor 2088604 in 2016\n",
      "Fetching data for sensor 2088548 in 2016\n",
      "Fetching data for sensor 2088589 in 2016\n",
      "Fetching data for sensor 2000821 in 2016\n",
      "Fetching data for sensor 2000905 in 2016\n",
      "Fetching data for sensor 2000768 in 2016\n",
      "Fetching data for sensor 2000525 in 2016\n",
      "Fetching data for sensor 2000753 in 2016\n",
      "Fetching data for sensor 2000561 in 2016\n",
      "Fetching data for sensor 2000676 in 2016\n",
      "Fetching data for sensor 2001076 in 2016\n",
      "Fetching data for sensor 2000935 in 2016\n",
      "Fetching data for sensor 2000945 in 2016\n",
      "Fetching data for sensor 2000750 in 2016\n",
      "Fetching data for sensor 2001039 in 2016\n",
      "Fetching data for sensor 2000499 in 2016\n",
      "Fetching data for sensor 2000798 in 2016\n",
      "Fetching data for sensor 2000553 in 2016\n",
      "Fetching data for sensor 2000565 in 2016\n",
      "Fetching data for sensor 2000718 in 2016\n",
      "Fetching data for sensor 2000977 in 2016\n",
      "Fetching data for sensor 2000505 in 2016\n",
      "Fetching data for sensor 2000853 in 2016\n",
      "Fetching data for sensor 2000522 in 2016\n",
      "Fetching data for sensor 2000723 in 2016\n",
      "Fetching data for sensor 2000819 in 2016\n",
      "Fetching data for sensor 2000729 in 2016\n",
      "Fetching data for sensor 2001006 in 2016\n",
      "Fetching data for sensor 2000981 in 2016\n",
      "Fetching data for sensor 2000667 in 2016\n",
      "Fetching data for sensor 2000840 in 2016\n",
      "Fetching data for sensor 2000567 in 2016\n",
      "Fetching data for sensor 2000963 in 2016\n",
      "Fetching data for sensor 2000537 in 2016\n",
      "Fetching data for sensor 2000471 in 2016\n",
      "Fetching data for sensor 2001038 in 2016\n",
      "Fetching data for sensor 2000956 in 2016\n",
      "Fetching data for sensor 2000694 in 2016\n",
      "Fetching data for sensor 2000953 in 2016\n",
      "Fetching data for sensor 2000629 in 2016\n",
      "Fetching data for sensor 2000671 in 2016\n",
      "Fetching data for sensor 2000967 in 2016\n",
      "Fetching data for sensor 2000999 in 2016\n",
      "Fetching data for sensor 2000660 in 2016\n",
      "Fetching data for sensor 2001013 in 2016\n",
      "Fetching data for sensor 2000613 in 2016\n",
      "Fetching data for sensor 2000483 in 2016\n",
      "Fetching data for sensor 2000550 in 2016\n",
      "Fetching data for sensor 2000832 in 2016\n",
      "Fetching data for sensor 2000796 in 2016\n",
      "Fetching data for sensor 2000934 in 2016\n",
      "Fetching data for sensor 2001040 in 2016\n",
      "Fetching data for sensor 2000838 in 2016\n",
      "Fetching data for sensor 2001062 in 2016\n",
      "Fetching data for sensor 2000933 in 2016\n",
      "Fetching data for sensor 2000443 in 2016\n",
      "Fetching data for sensor 2000993 in 2016\n",
      "Fetching data for sensor 2000942 in 2016\n",
      "Fetching data for sensor 2000852 in 2016\n",
      "Fetching data for sensor 2000445 in 2016\n",
      "Fetching data for sensor 2001033 in 2016\n",
      "Fetching data for sensor 2000984 in 2016\n",
      "Fetching data for sensor 2775 in 2017\n",
      "Fetching data for sensor 25551 in 2017\n",
      "Fetching data for sensor 15731 in 2017\n",
      "Fetching data for sensor 25196 in 2017\n",
      "Fetching data for sensor 24000 in 2017\n",
      "Fetching data for sensor 1654141 in 2017\n",
      "Fetching data for sensor 1654143 in 2017\n",
      "Fetching data for sensor 1654156 in 2017\n",
      "Fetching data for sensor 1654168 in 2017\n",
      "Fetching data for sensor 1654191 in 2017\n",
      "Fetching data for sensor 2000869 in 2017\n",
      "Fetching data for sensor 1654217 in 2017\n",
      "Fetching data for sensor 2000834 in 2017\n",
      "Fetching data for sensor 1654333 in 2017\n",
      "Fetching data for sensor 1654360 in 2017\n",
      "Fetching data for sensor 2000475 in 2017\n",
      "Fetching data for sensor 2000731 in 2017\n",
      "Fetching data for sensor 2000674 in 2017\n",
      "Fetching data for sensor 1654499 in 2017\n",
      "Fetching data for sensor 1654545 in 2017\n",
      "Fetching data for sensor 1654577 in 2017\n",
      "Fetching data for sensor 1654556 in 2017\n",
      "Fetching data for sensor 1654629 in 2017\n",
      "Fetching data for sensor 2000900 in 2017\n",
      "Fetching data for sensor 1999896 in 2017\n",
      "Fetching data for sensor 2001073 in 2017\n",
      "Fetching data for sensor 2001289 in 2017\n",
      "Fetching data for sensor 2001343 in 2017\n",
      "Fetching data for sensor 2088604 in 2017\n",
      "Fetching data for sensor 2088548 in 2017\n",
      "Fetching data for sensor 2088589 in 2017\n",
      "Fetching data for sensor 2000821 in 2017\n",
      "Fetching data for sensor 2000905 in 2017\n",
      "Fetching data for sensor 2000768 in 2017\n",
      "Fetching data for sensor 2000525 in 2017\n",
      "Fetching data for sensor 2000753 in 2017\n",
      "Fetching data for sensor 2000561 in 2017\n",
      "Fetching data for sensor 2000676 in 2017\n",
      "Fetching data for sensor 2001076 in 2017\n",
      "Fetching data for sensor 2000935 in 2017\n",
      "Fetching data for sensor 2000945 in 2017\n",
      "Fetching data for sensor 2000750 in 2017\n",
      "Fetching data for sensor 2001039 in 2017\n",
      "Fetching data for sensor 2000499 in 2017\n",
      "Fetching data for sensor 2000798 in 2017\n",
      "Fetching data for sensor 2000553 in 2017\n",
      "Fetching data for sensor 2000565 in 2017\n",
      "Fetching data for sensor 2000718 in 2017\n",
      "Fetching data for sensor 2000977 in 2017\n",
      "Fetching data for sensor 2000505 in 2017\n",
      "Fetching data for sensor 2000853 in 2017\n",
      "Fetching data for sensor 2000522 in 2017\n",
      "Fetching data for sensor 2000723 in 2017\n",
      "Fetching data for sensor 2000819 in 2017\n",
      "Fetching data for sensor 2000729 in 2017\n",
      "Fetching data for sensor 2001006 in 2017\n",
      "Fetching data for sensor 2000981 in 2017\n",
      "Fetching data for sensor 2000667 in 2017\n",
      "Fetching data for sensor 2000840 in 2017\n",
      "Fetching data for sensor 2000567 in 2017\n",
      "Fetching data for sensor 2000963 in 2017\n",
      "Fetching data for sensor 2000537 in 2017\n",
      "Fetching data for sensor 2000471 in 2017\n",
      "Fetching data for sensor 2001038 in 2017\n",
      "Fetching data for sensor 2000956 in 2017\n",
      "Fetching data for sensor 2000694 in 2017\n",
      "Fetching data for sensor 2000953 in 2017\n",
      "Fetching data for sensor 2000629 in 2017\n",
      "Fetching data for sensor 2000671 in 2017\n",
      "Fetching data for sensor 2000967 in 2017\n",
      "Fetching data for sensor 2000999 in 2017\n",
      "Fetching data for sensor 2000660 in 2017\n",
      "Fetching data for sensor 2001013 in 2017\n",
      "Fetching data for sensor 2000613 in 2017\n",
      "Fetching data for sensor 2000483 in 2017\n",
      "Fetching data for sensor 2000550 in 2017\n",
      "Fetching data for sensor 2000832 in 2017\n",
      "Fetching data for sensor 2000796 in 2017\n",
      "Fetching data for sensor 2000934 in 2017\n",
      "Fetching data for sensor 2001040 in 2017\n",
      "Fetching data for sensor 2000838 in 2017\n",
      "Fetching data for sensor 2001062 in 2017\n",
      "Fetching data for sensor 2000933 in 2017\n",
      "Fetching data for sensor 2000443 in 2017\n",
      "Fetching data for sensor 2000993 in 2017\n",
      "Fetching data for sensor 2000942 in 2017\n",
      "Fetching data for sensor 2000852 in 2017\n",
      "Fetching data for sensor 2000445 in 2017\n",
      "Fetching data for sensor 2001033 in 2017\n",
      "Fetching data for sensor 2000984 in 2017\n",
      "Fetching data for sensor 2775 in 2018\n",
      "Fetching data for sensor 25551 in 2018\n",
      "Fetching data for sensor 15731 in 2018\n",
      "Fetching data for sensor 25196 in 2018\n",
      "Fetching data for sensor 24000 in 2018\n",
      "Fetching data for sensor 1654141 in 2018\n",
      "Fetching data for sensor 1654143 in 2018\n",
      "Fetching data for sensor 1654156 in 2018\n",
      "Fetching data for sensor 1654168 in 2018\n",
      "Fetching data for sensor 1654191 in 2018\n",
      "Fetching data for sensor 2000869 in 2018\n",
      "Fetching data for sensor 1654217 in 2018\n",
      "Fetching data for sensor 2000834 in 2018\n",
      "Fetching data for sensor 1654333 in 2018\n",
      "Fetching data for sensor 1654360 in 2018\n",
      "Fetching data for sensor 2000475 in 2018\n",
      "Fetching data for sensor 2000731 in 2018\n",
      "Fetching data for sensor 2000674 in 2018\n",
      "Fetching data for sensor 1654499 in 2018\n",
      "Fetching data for sensor 1654545 in 2018\n",
      "Fetching data for sensor 1654577 in 2018\n",
      "Fetching data for sensor 1654556 in 2018\n",
      "Fetching data for sensor 1654629 in 2018\n",
      "Fetching data for sensor 2000900 in 2018\n",
      "Fetching data for sensor 1999896 in 2018\n",
      "Fetching data for sensor 2001073 in 2018\n",
      "Fetching data for sensor 2001289 in 2018\n",
      "Fetching data for sensor 2001343 in 2018\n",
      "Fetching data for sensor 2088604 in 2018\n",
      "Fetching data for sensor 2088548 in 2018\n",
      "Fetching data for sensor 2088589 in 2018\n",
      "Fetching data for sensor 2000821 in 2018\n",
      "Fetching data for sensor 2000905 in 2018\n",
      "Fetching data for sensor 2000768 in 2018\n",
      "Fetching data for sensor 2000525 in 2018\n",
      "Fetching data for sensor 2000753 in 2018\n",
      "Fetching data for sensor 2000561 in 2018\n",
      "Fetching data for sensor 2000676 in 2018\n",
      "Fetching data for sensor 2001076 in 2018\n",
      "Fetching data for sensor 2000935 in 2018\n",
      "Fetching data for sensor 2000945 in 2018\n",
      "Fetching data for sensor 2000750 in 2018\n",
      "Fetching data for sensor 2001039 in 2018\n",
      "Fetching data for sensor 2000499 in 2018\n",
      "Fetching data for sensor 2000798 in 2018\n",
      "Fetching data for sensor 2000553 in 2018\n",
      "Fetching data for sensor 2000565 in 2018\n",
      "Fetching data for sensor 2000718 in 2018\n",
      "Fetching data for sensor 2000977 in 2018\n",
      "Fetching data for sensor 2000505 in 2018\n",
      "Fetching data for sensor 2000853 in 2018\n",
      "Fetching data for sensor 2000522 in 2018\n",
      "Fetching data for sensor 2000723 in 2018\n",
      "Fetching data for sensor 2000819 in 2018\n",
      "Fetching data for sensor 2000729 in 2018\n",
      "Fetching data for sensor 2001006 in 2018\n",
      "Fetching data for sensor 2000981 in 2018\n",
      "Fetching data for sensor 2000667 in 2018\n",
      "Fetching data for sensor 2000840 in 2018\n",
      "Fetching data for sensor 2000567 in 2018\n",
      "Fetching data for sensor 2000963 in 2018\n",
      "Fetching data for sensor 2000537 in 2018\n",
      "Fetching data for sensor 2000471 in 2018\n",
      "Fetching data for sensor 2001038 in 2018\n",
      "Fetching data for sensor 2000956 in 2018\n",
      "Fetching data for sensor 2000694 in 2018\n",
      "Fetching data for sensor 2000953 in 2018\n",
      "Fetching data for sensor 2000629 in 2018\n",
      "Fetching data for sensor 2000671 in 2018\n",
      "Fetching data for sensor 2000967 in 2018\n",
      "Fetching data for sensor 2000999 in 2018\n",
      "Fetching data for sensor 2000660 in 2018\n",
      "Fetching data for sensor 2001013 in 2018\n",
      "Fetching data for sensor 2000613 in 2018\n",
      "Fetching data for sensor 2000483 in 2018\n",
      "Fetching data for sensor 2000550 in 2018\n",
      "Fetching data for sensor 2000832 in 2018\n",
      "Fetching data for sensor 2000796 in 2018\n",
      "Fetching data for sensor 2000934 in 2018\n",
      "Fetching data for sensor 2001040 in 2018\n",
      "Fetching data for sensor 2000838 in 2018\n",
      "Fetching data for sensor 2001062 in 2018\n",
      "Fetching data for sensor 2000933 in 2018\n",
      "Fetching data for sensor 2000443 in 2018\n",
      "Fetching data for sensor 2000993 in 2018\n",
      "Fetching data for sensor 2000942 in 2018\n",
      "Fetching data for sensor 2000852 in 2018\n",
      "Fetching data for sensor 2000445 in 2018\n",
      "Fetching data for sensor 2001033 in 2018\n",
      "Fetching data for sensor 2000984 in 2018\n",
      "Fetching data for sensor 2775 in 2019\n",
      "Fetching data for sensor 25551 in 2019\n",
      "Fetching data for sensor 15731 in 2019\n",
      "Fetching data for sensor 25196 in 2019\n",
      "Fetching data for sensor 24000 in 2019\n",
      "Fetching data for sensor 1654141 in 2019\n",
      "Fetching data for sensor 1654143 in 2019\n",
      "Fetching data for sensor 1654156 in 2019\n",
      "Fetching data for sensor 1654168 in 2019\n",
      "Fetching data for sensor 1654191 in 2019\n",
      "Fetching data for sensor 2000869 in 2019\n",
      "Fetching data for sensor 1654217 in 2019\n",
      "Fetching data for sensor 2000834 in 2019\n",
      "Fetching data for sensor 1654333 in 2019\n",
      "Fetching data for sensor 1654360 in 2019\n",
      "Fetching data for sensor 2000475 in 2019\n",
      "Fetching data for sensor 2000731 in 2019\n",
      "Fetching data for sensor 2000674 in 2019\n",
      "Fetching data for sensor 1654499 in 2019\n",
      "Fetching data for sensor 1654545 in 2019\n",
      "Fetching data for sensor 1654577 in 2019\n",
      "Fetching data for sensor 1654556 in 2019\n",
      "Fetching data for sensor 1654629 in 2019\n",
      "Fetching data for sensor 2000900 in 2019\n",
      "Fetching data for sensor 1999896 in 2019\n",
      "Fetching data for sensor 2001073 in 2019\n",
      "Fetching data for sensor 2001289 in 2019\n",
      "Fetching data for sensor 2001343 in 2019\n",
      "Fetching data for sensor 2088604 in 2019\n",
      "Fetching data for sensor 2088548 in 2019\n",
      "Fetching data for sensor 2088589 in 2019\n",
      "Fetching data for sensor 2000821 in 2019\n",
      "Fetching data for sensor 2000905 in 2019\n",
      "Fetching data for sensor 2000768 in 2019\n",
      "Fetching data for sensor 2000525 in 2019\n",
      "Fetching data for sensor 2000753 in 2019\n",
      "Fetching data for sensor 2000561 in 2019\n",
      "Fetching data for sensor 2000676 in 2019\n",
      "Fetching data for sensor 2001076 in 2019\n",
      "Fetching data for sensor 2000935 in 2019\n",
      "Fetching data for sensor 2000945 in 2019\n",
      "Fetching data for sensor 2000750 in 2019\n",
      "Fetching data for sensor 2001039 in 2019\n",
      "Fetching data for sensor 2000499 in 2019\n",
      "Fetching data for sensor 2000798 in 2019\n",
      "Fetching data for sensor 2000553 in 2019\n",
      "Fetching data for sensor 2000565 in 2019\n",
      "Fetching data for sensor 2000718 in 2019\n",
      "Fetching data for sensor 2000977 in 2019\n",
      "Fetching data for sensor 2000505 in 2019\n",
      "Fetching data for sensor 2000853 in 2019\n",
      "Fetching data for sensor 2000522 in 2019\n",
      "Fetching data for sensor 2000723 in 2019\n",
      "Fetching data for sensor 2000819 in 2019\n",
      "Fetching data for sensor 2000729 in 2019\n",
      "Fetching data for sensor 2001006 in 2019\n",
      "Fetching data for sensor 2000981 in 2019\n",
      "Fetching data for sensor 2000667 in 2019\n",
      "Fetching data for sensor 2000840 in 2019\n",
      "Fetching data for sensor 2000567 in 2019\n",
      "Fetching data for sensor 2000963 in 2019\n",
      "Fetching data for sensor 2000537 in 2019\n",
      "Fetching data for sensor 2000471 in 2019\n",
      "Fetching data for sensor 2001038 in 2019\n",
      "Fetching data for sensor 2000956 in 2019\n",
      "Fetching data for sensor 2000694 in 2019\n",
      "Fetching data for sensor 2000953 in 2019\n",
      "Fetching data for sensor 2000629 in 2019\n",
      "Fetching data for sensor 2000671 in 2019\n",
      "Fetching data for sensor 2000967 in 2019\n",
      "Fetching data for sensor 2000999 in 2019\n",
      "Fetching data for sensor 2000660 in 2019\n",
      "Fetching data for sensor 2001013 in 2019\n",
      "Fetching data for sensor 2000613 in 2019\n",
      "Fetching data for sensor 2000483 in 2019\n",
      "Fetching data for sensor 2000550 in 2019\n",
      "Fetching data for sensor 2000832 in 2019\n",
      "Fetching data for sensor 2000796 in 2019\n",
      "Fetching data for sensor 2000934 in 2019\n",
      "Fetching data for sensor 2001040 in 2019\n",
      "Fetching data for sensor 2000838 in 2019\n",
      "Fetching data for sensor 2001062 in 2019\n",
      "Fetching data for sensor 2000933 in 2019\n",
      "Fetching data for sensor 2000443 in 2019\n",
      "Fetching data for sensor 2000993 in 2019\n",
      "Fetching data for sensor 2000942 in 2019\n",
      "Fetching data for sensor 2000852 in 2019\n",
      "Fetching data for sensor 2000445 in 2019\n",
      "Fetching data for sensor 2001033 in 2019\n",
      "Fetching data for sensor 2000984 in 2019\n",
      "Fetching data for sensor 2775 in 2020\n",
      "Fetching data for sensor 25551 in 2020\n",
      "Fetching data for sensor 15731 in 2020\n",
      "Fetching data for sensor 25196 in 2020\n",
      "Fetching data for sensor 24000 in 2020\n",
      "Fetching data for sensor 1654141 in 2020\n",
      "Fetching data for sensor 1654143 in 2020\n",
      "Fetching data for sensor 1654156 in 2020\n",
      "Fetching data for sensor 1654168 in 2020\n",
      "Fetching data for sensor 1654191 in 2020\n",
      "Fetching data for sensor 2000869 in 2020\n",
      "Fetching data for sensor 1654217 in 2020\n",
      "Fetching data for sensor 2000834 in 2020\n",
      "Fetching data for sensor 1654333 in 2020\n",
      "Fetching data for sensor 1654360 in 2020\n",
      "Fetching data for sensor 2000475 in 2020\n",
      "Fetching data for sensor 2000731 in 2020\n",
      "Fetching data for sensor 2000674 in 2020\n",
      "Fetching data for sensor 1654499 in 2020\n",
      "Fetching data for sensor 1654545 in 2020\n",
      "Fetching data for sensor 1654577 in 2020\n",
      "Fetching data for sensor 1654556 in 2020\n",
      "Fetching data for sensor 1654629 in 2020\n",
      "Fetching data for sensor 2000900 in 2020\n",
      "Fetching data for sensor 1999896 in 2020\n",
      "Fetching data for sensor 2001073 in 2020\n",
      "Fetching data for sensor 2001289 in 2020\n",
      "Fetching data for sensor 2001343 in 2020\n",
      "Fetching data for sensor 2088604 in 2020\n",
      "Fetching data for sensor 2088548 in 2020\n",
      "Fetching data for sensor 2088589 in 2020\n",
      "Fetching data for sensor 2000821 in 2020\n",
      "Fetching data for sensor 2000905 in 2020\n",
      "Fetching data for sensor 2000768 in 2020\n",
      "Fetching data for sensor 2000525 in 2020\n",
      "Fetching data for sensor 2000753 in 2020\n",
      "Fetching data for sensor 2000561 in 2020\n",
      "Fetching data for sensor 2000676 in 2020\n",
      "Fetching data for sensor 2001076 in 2020\n",
      "Fetching data for sensor 2000935 in 2020\n",
      "Fetching data for sensor 2000945 in 2020\n",
      "Fetching data for sensor 2000750 in 2020\n",
      "Fetching data for sensor 2001039 in 2020\n",
      "Fetching data for sensor 2000499 in 2020\n",
      "Fetching data for sensor 2000798 in 2020\n",
      "Fetching data for sensor 2000553 in 2020\n",
      "Fetching data for sensor 2000565 in 2020\n",
      "Fetching data for sensor 2000718 in 2020\n",
      "Fetching data for sensor 2000977 in 2020\n",
      "Fetching data for sensor 2000505 in 2020\n",
      "Fetching data for sensor 2000853 in 2020\n",
      "Fetching data for sensor 2000522 in 2020\n",
      "Fetching data for sensor 2000723 in 2020\n",
      "Fetching data for sensor 2000819 in 2020\n",
      "Fetching data for sensor 2000729 in 2020\n",
      "Fetching data for sensor 2001006 in 2020\n",
      "Fetching data for sensor 2000981 in 2020\n",
      "Fetching data for sensor 2000667 in 2020\n",
      "Fetching data for sensor 2000840 in 2020\n",
      "Fetching data for sensor 2000567 in 2020\n",
      "Fetching data for sensor 2000963 in 2020\n",
      "Fetching data for sensor 2000537 in 2020\n",
      "Fetching data for sensor 2000471 in 2020\n",
      "Fetching data for sensor 2001038 in 2020\n",
      "Fetching data for sensor 2000956 in 2020\n",
      "Fetching data for sensor 2000694 in 2020\n",
      "Fetching data for sensor 2000953 in 2020\n",
      "Fetching data for sensor 2000629 in 2020\n",
      "Fetching data for sensor 2000671 in 2020\n",
      "Fetching data for sensor 2000967 in 2020\n",
      "Fetching data for sensor 2000999 in 2020\n",
      "Fetching data for sensor 2000660 in 2020\n",
      "Fetching data for sensor 2001013 in 2020\n",
      "Fetching data for sensor 2000613 in 2020\n",
      "Fetching data for sensor 2000483 in 2020\n",
      "Fetching data for sensor 2000550 in 2020\n",
      "Fetching data for sensor 2000832 in 2020\n",
      "Fetching data for sensor 2000796 in 2020\n",
      "Fetching data for sensor 2000934 in 2020\n",
      "Fetching data for sensor 2001040 in 2020\n",
      "Fetching data for sensor 2000838 in 2020\n",
      "Fetching data for sensor 2001062 in 2020\n",
      "Fetching data for sensor 2000933 in 2020\n",
      "Fetching data for sensor 2000443 in 2020\n",
      "Fetching data for sensor 2000993 in 2020\n",
      "Fetching data for sensor 2000942 in 2020\n",
      "Fetching data for sensor 2000852 in 2020\n",
      "Fetching data for sensor 2000445 in 2020\n",
      "Fetching data for sensor 2001033 in 2020\n",
      "Fetching data for sensor 2000984 in 2020\n",
      "Fetching data for sensor 2775 in 2021\n",
      "Fetching data for sensor 25551 in 2021\n",
      "Fetching data for sensor 15731 in 2021\n",
      "Fetching data for sensor 25196 in 2021\n",
      "Fetching data for sensor 24000 in 2021\n",
      "Fetching data for sensor 1654141 in 2021\n",
      "Fetching data for sensor 1654143 in 2021\n",
      "Fetching data for sensor 1654156 in 2021\n",
      "Fetching data for sensor 1654168 in 2021\n",
      "Fetching data for sensor 1654191 in 2021\n",
      "Fetching data for sensor 2000869 in 2021\n",
      "Fetching data for sensor 1654217 in 2021\n",
      "Fetching data for sensor 2000834 in 2021\n",
      "Fetching data for sensor 1654333 in 2021\n",
      "Fetching data for sensor 1654360 in 2021\n",
      "Fetching data for sensor 2000475 in 2021\n",
      "Fetching data for sensor 2000731 in 2021\n",
      "Fetching data for sensor 2000674 in 2021\n",
      "Fetching data for sensor 1654499 in 2021\n",
      "Fetching data for sensor 1654545 in 2021\n",
      "Fetching data for sensor 1654577 in 2021\n",
      "Fetching data for sensor 1654556 in 2021\n",
      "Fetching data for sensor 1654629 in 2021\n",
      "Fetching data for sensor 2000900 in 2021\n",
      "Fetching data for sensor 1999896 in 2021\n",
      "Fetching data for sensor 2001073 in 2021\n",
      "Fetching data for sensor 2001289 in 2021\n",
      "Fetching data for sensor 2001343 in 2021\n",
      "Fetching data for sensor 2088604 in 2021\n",
      "Fetching data for sensor 2088548 in 2021\n",
      "Fetching data for sensor 2088589 in 2021\n",
      "Fetching data for sensor 2000821 in 2021\n",
      "Fetching data for sensor 2000905 in 2021\n",
      "Fetching data for sensor 2000768 in 2021\n",
      "Fetching data for sensor 2000525 in 2021\n",
      "Fetching data for sensor 2000753 in 2021\n",
      "Fetching data for sensor 2000561 in 2021\n",
      "Fetching data for sensor 2000676 in 2021\n",
      "Fetching data for sensor 2001076 in 2021\n",
      "Fetching data for sensor 2000935 in 2021\n",
      "Fetching data for sensor 2000945 in 2021\n",
      "Fetching data for sensor 2000750 in 2021\n",
      "Fetching data for sensor 2001039 in 2021\n",
      "Fetching data for sensor 2000499 in 2021\n",
      "Fetching data for sensor 2000798 in 2021\n",
      "Fetching data for sensor 2000553 in 2021\n",
      "Fetching data for sensor 2000565 in 2021\n",
      "Fetching data for sensor 2000718 in 2021\n",
      "Fetching data for sensor 2000977 in 2021\n",
      "Fetching data for sensor 2000505 in 2021\n",
      "Fetching data for sensor 2000853 in 2021\n",
      "Fetching data for sensor 2000522 in 2021\n",
      "Fetching data for sensor 2000723 in 2021\n",
      "Fetching data for sensor 2000819 in 2021\n",
      "Fetching data for sensor 2000729 in 2021\n",
      "Fetching data for sensor 2001006 in 2021\n",
      "Fetching data for sensor 2000981 in 2021\n",
      "Fetching data for sensor 2000667 in 2021\n",
      "Fetching data for sensor 2000840 in 2021\n",
      "Fetching data for sensor 2000567 in 2021\n",
      "Fetching data for sensor 2000963 in 2021\n",
      "Fetching data for sensor 2000537 in 2021\n",
      "Fetching data for sensor 2000471 in 2021\n",
      "Fetching data for sensor 2001038 in 2021\n",
      "Fetching data for sensor 2000956 in 2021\n",
      "Fetching data for sensor 2000694 in 2021\n",
      "Fetching data for sensor 2000953 in 2021\n",
      "Fetching data for sensor 2000629 in 2021\n",
      "Fetching data for sensor 2000671 in 2021\n",
      "Fetching data for sensor 2000967 in 2021\n",
      "Fetching data for sensor 2000999 in 2021\n",
      "Fetching data for sensor 2000660 in 2021\n",
      "Fetching data for sensor 2001013 in 2021\n",
      "Fetching data for sensor 2000613 in 2021\n",
      "Fetching data for sensor 2000483 in 2021\n",
      "Fetching data for sensor 2000550 in 2021\n",
      "Fetching data for sensor 2000832 in 2021\n",
      "Fetching data for sensor 2000796 in 2021\n",
      "Fetching data for sensor 2000934 in 2021\n",
      "Fetching data for sensor 2001040 in 2021\n",
      "Fetching data for sensor 2000838 in 2021\n",
      "Fetching data for sensor 2001062 in 2021\n",
      "Fetching data for sensor 2000933 in 2021\n",
      "Fetching data for sensor 2000443 in 2021\n",
      "Fetching data for sensor 2000993 in 2021\n",
      "Fetching data for sensor 2000942 in 2021\n",
      "Fetching data for sensor 2000852 in 2021\n",
      "Fetching data for sensor 2000445 in 2021\n",
      "Fetching data for sensor 2001033 in 2021\n",
      "Fetching data for sensor 2000984 in 2021\n",
      "Fetching data for sensor 2775 in 2022\n",
      "Fetching data for sensor 25551 in 2022\n",
      "Fetching data for sensor 15731 in 2022\n",
      "Fetching data for sensor 25196 in 2022\n",
      "Fetching data for sensor 24000 in 2022\n",
      "Fetching data for sensor 1654141 in 2022\n",
      "Fetching data for sensor 1654143 in 2022\n",
      "Fetching data for sensor 1654156 in 2022\n",
      "Fetching data for sensor 1654168 in 2022\n",
      "Fetching data for sensor 1654191 in 2022\n",
      "Fetching data for sensor 2000869 in 2022\n",
      "Fetching data for sensor 1654217 in 2022\n",
      "Fetching data for sensor 2000834 in 2022\n",
      "Fetching data for sensor 1654333 in 2022\n",
      "Fetching data for sensor 1654360 in 2022\n",
      "Fetching data for sensor 2000475 in 2022\n",
      "Fetching data for sensor 2000731 in 2022\n",
      "Fetching data for sensor 2000674 in 2022\n",
      "Fetching data for sensor 1654499 in 2022\n",
      "Fetching data for sensor 1654545 in 2022\n",
      "Fetching data for sensor 1654577 in 2022\n",
      "Fetching data for sensor 1654556 in 2022\n",
      "Fetching data for sensor 1654629 in 2022\n",
      "Fetching data for sensor 2000900 in 2022\n",
      "Fetching data for sensor 1999896 in 2022\n",
      "Fetching data for sensor 2001073 in 2022\n",
      "Fetching data for sensor 2001289 in 2022\n",
      "Fetching data for sensor 2001343 in 2022\n",
      "Fetching data for sensor 2088604 in 2022\n",
      "Fetching data for sensor 2088548 in 2022\n",
      "Fetching data for sensor 2088589 in 2022\n",
      "Fetching data for sensor 2000821 in 2022\n",
      "Fetching data for sensor 2000905 in 2022\n",
      "Fetching data for sensor 2000768 in 2022\n",
      "Fetching data for sensor 2000525 in 2022\n",
      "Fetching data for sensor 2000753 in 2022\n",
      "Fetching data for sensor 2000561 in 2022\n",
      "Fetching data for sensor 2000676 in 2022\n",
      "Fetching data for sensor 2001076 in 2022\n",
      "Fetching data for sensor 2000935 in 2022\n",
      "Fetching data for sensor 2000945 in 2022\n",
      "Fetching data for sensor 2000750 in 2022\n",
      "Fetching data for sensor 2001039 in 2022\n",
      "Fetching data for sensor 2000499 in 2022\n",
      "Fetching data for sensor 2000798 in 2022\n",
      "Fetching data for sensor 2000553 in 2022\n",
      "Fetching data for sensor 2000565 in 2022\n",
      "Fetching data for sensor 2000718 in 2022\n",
      "Fetching data for sensor 2000977 in 2022\n",
      "Fetching data for sensor 2000505 in 2022\n",
      "Fetching data for sensor 2000853 in 2022\n",
      "Fetching data for sensor 2000522 in 2022\n",
      "Fetching data for sensor 2000723 in 2022\n",
      "Fetching data for sensor 2000819 in 2022\n",
      "Fetching data for sensor 2000729 in 2022\n",
      "Fetching data for sensor 2001006 in 2022\n",
      "Fetching data for sensor 2000981 in 2022\n",
      "Fetching data for sensor 2000667 in 2022\n",
      "Fetching data for sensor 2000840 in 2022\n",
      "Fetching data for sensor 2000567 in 2022\n",
      "Fetching data for sensor 2000963 in 2022\n",
      "Fetching data for sensor 2000537 in 2022\n",
      "Fetching data for sensor 2000471 in 2022\n",
      "Fetching data for sensor 2001038 in 2022\n",
      "Fetching data for sensor 2000956 in 2022\n",
      "Fetching data for sensor 2000694 in 2022\n",
      "Fetching data for sensor 2000953 in 2022\n",
      "Fetching data for sensor 2000629 in 2022\n",
      "Fetching data for sensor 2000671 in 2022\n",
      "Fetching data for sensor 2000967 in 2022\n",
      "Fetching data for sensor 2000999 in 2022\n",
      "Fetching data for sensor 2000660 in 2022\n",
      "Fetching data for sensor 2001013 in 2022\n",
      "Fetching data for sensor 2000613 in 2022\n",
      "Fetching data for sensor 2000483 in 2022\n",
      "Fetching data for sensor 2000550 in 2022\n",
      "Fetching data for sensor 2000832 in 2022\n",
      "Fetching data for sensor 2000796 in 2022\n",
      "Fetching data for sensor 2000934 in 2022\n",
      "Fetching data for sensor 2001040 in 2022\n",
      "Fetching data for sensor 2000838 in 2022\n",
      "Fetching data for sensor 2001062 in 2022\n",
      "Fetching data for sensor 2000933 in 2022\n",
      "Fetching data for sensor 2000443 in 2022\n",
      "Fetching data for sensor 2000993 in 2022\n",
      "Fetching data for sensor 2000942 in 2022\n",
      "Fetching data for sensor 2000852 in 2022\n",
      "Fetching data for sensor 2000445 in 2022\n",
      "Fetching data for sensor 2001033 in 2022\n",
      "Fetching data for sensor 2000984 in 2022\n",
      "Fetching data for sensor 2775 in 2023\n",
      "Fetching data for sensor 25551 in 2023\n",
      "Fetching data for sensor 15731 in 2023\n",
      "Fetching data for sensor 25196 in 2023\n",
      "Fetching data for sensor 24000 in 2023\n",
      "Fetching data for sensor 1654141 in 2023\n",
      "Fetching data for sensor 1654143 in 2023\n",
      "Fetching data for sensor 1654156 in 2023\n",
      "Fetching data for sensor 1654168 in 2023\n",
      "Fetching data for sensor 1654191 in 2023\n",
      "Fetching data for sensor 2000869 in 2023\n",
      "Fetching data for sensor 1654217 in 2023\n",
      "Fetching data for sensor 2000834 in 2023\n",
      "Fetching data for sensor 1654333 in 2023\n",
      "Fetching data for sensor 1654360 in 2023\n",
      "Fetching data for sensor 2000475 in 2023\n",
      "Fetching data for sensor 2000731 in 2023\n",
      "Fetching data for sensor 2000674 in 2023\n",
      "Fetching data for sensor 1654499 in 2023\n",
      "Fetching data for sensor 1654545 in 2023\n",
      "Fetching data for sensor 1654577 in 2023\n",
      "Fetching data for sensor 1654556 in 2023\n",
      "Fetching data for sensor 1654629 in 2023\n",
      "Fetching data for sensor 2000900 in 2023\n",
      "Fetching data for sensor 1999896 in 2023\n",
      "Fetching data for sensor 2001073 in 2023\n",
      "Fetching data for sensor 2001289 in 2023\n",
      "Fetching data for sensor 2001343 in 2023\n",
      "Fetching data for sensor 2088604 in 2023\n",
      "Fetching data for sensor 2088548 in 2023\n",
      "Fetching data for sensor 2088589 in 2023\n",
      "Fetching data for sensor 2000821 in 2023\n",
      "Fetching data for sensor 2000905 in 2023\n",
      "Fetching data for sensor 2000768 in 2023\n",
      "Fetching data for sensor 2000525 in 2023\n",
      "Fetching data for sensor 2000753 in 2023\n",
      "Fetching data for sensor 2000561 in 2023\n",
      "Fetching data for sensor 2000676 in 2023\n",
      "Fetching data for sensor 2001076 in 2023\n",
      "Fetching data for sensor 2000935 in 2023\n",
      "Fetching data for sensor 2000945 in 2023\n",
      "Fetching data for sensor 2000750 in 2023\n",
      "Fetching data for sensor 2001039 in 2023\n",
      "Fetching data for sensor 2000499 in 2023\n",
      "Fetching data for sensor 2000798 in 2023\n",
      "Fetching data for sensor 2000553 in 2023\n",
      "Fetching data for sensor 2000565 in 2023\n",
      "Fetching data for sensor 2000718 in 2023\n",
      "Fetching data for sensor 2000977 in 2023\n",
      "Fetching data for sensor 2000505 in 2023\n",
      "Fetching data for sensor 2000853 in 2023\n",
      "Fetching data for sensor 2000522 in 2023\n",
      "Fetching data for sensor 2000723 in 2023\n",
      "Fetching data for sensor 2000819 in 2023\n",
      "Fetching data for sensor 2000729 in 2023\n",
      "Fetching data for sensor 2001006 in 2023\n",
      "Fetching data for sensor 2000981 in 2023\n",
      "Fetching data for sensor 2000667 in 2023\n",
      "Fetching data for sensor 2000840 in 2023\n",
      "Fetching data for sensor 2000567 in 2023\n",
      "Fetching data for sensor 2000963 in 2023\n",
      "Fetching data for sensor 2000537 in 2023\n",
      "Fetching data for sensor 2000471 in 2023\n",
      "Fetching data for sensor 2001038 in 2023\n",
      "Fetching data for sensor 2000956 in 2023\n",
      "Fetching data for sensor 2000694 in 2023\n",
      "Fetching data for sensor 2000953 in 2023\n",
      "Fetching data for sensor 2000629 in 2023\n",
      "Fetching data for sensor 2000671 in 2023\n",
      "Fetching data for sensor 2000967 in 2023\n",
      "Fetching data for sensor 2000999 in 2023\n",
      "Fetching data for sensor 2000660 in 2023\n",
      "Fetching data for sensor 2001013 in 2023\n",
      "Fetching data for sensor 2000613 in 2023\n",
      "Fetching data for sensor 2000483 in 2023\n",
      "Fetching data for sensor 2000550 in 2023\n",
      "Fetching data for sensor 2000832 in 2023\n",
      "Fetching data for sensor 2000796 in 2023\n",
      "Fetching data for sensor 2000934 in 2023\n",
      "Fetching data for sensor 2001040 in 2023\n",
      "Fetching data for sensor 2000838 in 2023\n",
      "Fetching data for sensor 2001062 in 2023\n",
      "Fetching data for sensor 2000933 in 2023\n",
      "Fetching data for sensor 2000443 in 2023\n",
      "Fetching data for sensor 2000993 in 2023\n",
      "Fetching data for sensor 2000942 in 2023\n",
      "Fetching data for sensor 2000852 in 2023\n",
      "Fetching data for sensor 2000445 in 2023\n",
      "Fetching data for sensor 2001033 in 2023\n",
      "Fetching data for sensor 2000984 in 2023\n",
      "Fetching data for sensor 2775 in 2024\n",
      "Fetching data for sensor 25551 in 2024\n",
      "Fetching data for sensor 15731 in 2024\n",
      "Fetching data for sensor 25196 in 2024\n",
      "Fetching data for sensor 24000 in 2024\n",
      "Fetching data for sensor 1654141 in 2024\n",
      "Fetching data for sensor 1654143 in 2024\n",
      "Fetching data for sensor 1654156 in 2024\n",
      "Fetching data for sensor 1654168 in 2024\n",
      "Fetching data for sensor 1654191 in 2024\n",
      "Fetching data for sensor 2000869 in 2024\n",
      "Fetching data for sensor 1654217 in 2024\n",
      "Fetching data for sensor 2000834 in 2024\n",
      "Fetching data for sensor 1654333 in 2024\n",
      "Fetching data for sensor 1654360 in 2024\n",
      "Fetching data for sensor 2000475 in 2024\n",
      "Fetching data for sensor 2000731 in 2024\n",
      "Fetching data for sensor 2000674 in 2024\n",
      "Fetching data for sensor 1654499 in 2024\n",
      "Fetching data for sensor 1654545 in 2024\n",
      "Fetching data for sensor 1654577 in 2024\n",
      "Fetching data for sensor 1654556 in 2024\n",
      "Fetching data for sensor 1654629 in 2024\n",
      "Fetching data for sensor 2000900 in 2024\n",
      "Fetching data for sensor 1999896 in 2024\n",
      "Fetching data for sensor 2001073 in 2024\n",
      "Fetching data for sensor 2001289 in 2024\n",
      "Fetching data for sensor 2001343 in 2024\n",
      "Fetching data for sensor 2088604 in 2024\n",
      "Fetching data for sensor 2088548 in 2024\n",
      "Fetching data for sensor 2088589 in 2024\n",
      "Fetching data for sensor 2000821 in 2024\n",
      "Fetching data for sensor 2000905 in 2024\n",
      "Fetching data for sensor 2000768 in 2024\n",
      "Fetching data for sensor 2000525 in 2024\n",
      "Fetching data for sensor 2000753 in 2024\n",
      "Fetching data for sensor 2000561 in 2024\n",
      "Fetching data for sensor 2000676 in 2024\n",
      "Fetching data for sensor 2001076 in 2024\n",
      "Fetching data for sensor 2000935 in 2024\n",
      "Fetching data for sensor 2000945 in 2024\n",
      "Fetching data for sensor 2000750 in 2024\n",
      "Fetching data for sensor 2001039 in 2024\n",
      "Fetching data for sensor 2000499 in 2024\n",
      "Fetching data for sensor 2000798 in 2024\n",
      "Fetching data for sensor 2000553 in 2024\n",
      "Fetching data for sensor 2000565 in 2024\n",
      "Fetching data for sensor 2000718 in 2024\n",
      "Fetching data for sensor 2000977 in 2024\n",
      "Fetching data for sensor 2000505 in 2024\n",
      "Fetching data for sensor 2000853 in 2024\n",
      "Fetching data for sensor 2000522 in 2024\n",
      "Fetching data for sensor 2000723 in 2024\n",
      "Fetching data for sensor 2000819 in 2024\n",
      "Fetching data for sensor 2000729 in 2024\n",
      "Fetching data for sensor 2001006 in 2024\n",
      "Fetching data for sensor 2000981 in 2024\n",
      "Fetching data for sensor 2000667 in 2024\n",
      "Fetching data for sensor 2000840 in 2024\n",
      "Fetching data for sensor 2000567 in 2024\n",
      "Fetching data for sensor 2000963 in 2024\n",
      "Fetching data for sensor 2000537 in 2024\n",
      "Fetching data for sensor 2000471 in 2024\n",
      "Fetching data for sensor 2001038 in 2024\n",
      "Fetching data for sensor 2000956 in 2024\n",
      "Fetching data for sensor 2000694 in 2024\n",
      "Fetching data for sensor 2000953 in 2024\n",
      "Fetching data for sensor 2000629 in 2024\n",
      "Fetching data for sensor 2000671 in 2024\n",
      "Fetching data for sensor 2000967 in 2024\n",
      "Fetching data for sensor 2000999 in 2024\n",
      "Fetching data for sensor 2000660 in 2024\n",
      "Fetching data for sensor 2001013 in 2024\n",
      "Fetching data for sensor 2000613 in 2024\n",
      "Fetching data for sensor 2000483 in 2024\n",
      "Fetching data for sensor 2000550 in 2024\n",
      "Fetching data for sensor 2000832 in 2024\n",
      "Fetching data for sensor 2000796 in 2024\n",
      "Fetching data for sensor 2000934 in 2024\n",
      "Fetching data for sensor 2001040 in 2024\n",
      "Fetching data for sensor 2000838 in 2024\n",
      "Fetching data for sensor 2001062 in 2024\n",
      "Fetching data for sensor 2000933 in 2024\n",
      "Fetching data for sensor 2000443 in 2024\n",
      "Fetching data for sensor 2000993 in 2024\n",
      "Fetching data for sensor 2000942 in 2024\n",
      "Fetching data for sensor 2000852 in 2024\n",
      "Fetching data for sensor 2000445 in 2024\n",
      "Fetching data for sensor 2001033 in 2024\n",
      "Fetching data for sensor 2000984 in 2024\n",
      "             day        avg  isUnhealthy\n",
      "0     2016-03-06   8.000000            0\n",
      "1     2016-03-07   4.500000            0\n",
      "2     2016-03-10  15.000000            1\n",
      "3     2016-03-11   9.080000            0\n",
      "4     2016-03-12   7.860000            0\n",
      "...          ...        ...          ...\n",
      "1978  2024-12-27  15.900000            1\n",
      "1979  2024-12-28  23.159091            1\n",
      "1980  2024-12-29  24.290909            1\n",
      "1981  2024-12-30  25.827273            1\n",
      "1982  2024-12-31  30.718182            1\n",
      "\n",
      "[1983 rows x 3 columns]\n",
      "aq_df.shape = (1983, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>day</th>\n",
       "      <th>avg</th>\n",
       "      <th>isUnhealthy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-03-06</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-03-07</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-03-10</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-03-11</td>\n",
       "      <td>9.080000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-03-12</td>\n",
       "      <td>7.860000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1978</th>\n",
       "      <td>2024-12-27</td>\n",
       "      <td>15.900000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1979</th>\n",
       "      <td>2024-12-28</td>\n",
       "      <td>23.159091</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980</th>\n",
       "      <td>2024-12-29</td>\n",
       "      <td>24.290909</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981</th>\n",
       "      <td>2024-12-30</td>\n",
       "      <td>25.827273</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1982</th>\n",
       "      <td>2024-12-31</td>\n",
       "      <td>30.718182</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1983 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             day        avg  isUnhealthy\n",
       "0     2016-03-06   8.000000            0\n",
       "1     2016-03-07   4.500000            0\n",
       "2     2016-03-10  15.000000            1\n",
       "3     2016-03-11   9.080000            0\n",
       "4     2016-03-12   7.860000            0\n",
       "...          ...        ...          ...\n",
       "1978  2024-12-27  15.900000            1\n",
       "1979  2024-12-28  23.159091            1\n",
       "1980  2024-12-29  24.290909            1\n",
       "1981  2024-12-30  25.827273            1\n",
       "1982  2024-12-31  30.718182            1\n",
       "\n",
       "[1983 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 63] File name too long: 'dataOpenAQ_los-angeles_pm25_2016-2024_2775-25551-15731-25196-24000-1654141-1654143-1654156-1654168-1654191-2000869-1654217-2000834-1654333-1654360-2000475-2000731-2000674-1654499-1654545-1654577-1654556-1654629-2000900-1999896-2001073-2001289-2001343-2088604-2088548-2088589-2000821-2000905-2000768-2000525-2000753-2000561-2000676-2001076-2000935-2000945-2000750-2001039-2000499-2000798-2000553-2000565-2000718-2000977-2000505-2000853-2000522-2000723-2000819-2000729-2001006-2000981-2000667-2000840-2000567-2000963-2000537-2000471-2001038-2000956-2000694-2000953-2000629-2000671-2000967-2000999-2000660-2001013-2000613-2000483-2000550-2000832-2000796-2000934-2001040-2000838-2001062-2000933-2000443-2000993-2000942-2000852-2000445-2001033-2000984.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maq_df.shape =\u001b[39m\u001b[38;5;124m'\u001b[39m, aq_df\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      9\u001b[0m display(aq_df)\n\u001b[0;32m---> 10\u001b[0m \u001b[43maq_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mAQbyWeather\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetFilenameOpenAQ\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/util/_decorators.py:333\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    328\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    329\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    330\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    331\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    332\u001b[0m     )\n\u001b[0;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/generic.py:3967\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[1;32m   3956\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_frame()\n\u001b[1;32m   3958\u001b[0m formatter \u001b[38;5;241m=\u001b[39m DataFrameFormatter(\n\u001b[1;32m   3959\u001b[0m     frame\u001b[38;5;241m=\u001b[39mdf,\n\u001b[1;32m   3960\u001b[0m     header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3964\u001b[0m     decimal\u001b[38;5;241m=\u001b[39mdecimal,\n\u001b[1;32m   3965\u001b[0m )\n\u001b[0;32m-> 3967\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameRenderer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3968\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3969\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlineterminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3970\u001b[0m \u001b[43m    \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3971\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3972\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3973\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3974\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquoting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquoting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3975\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3976\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3977\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3978\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3979\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquotechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquotechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3980\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3981\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoublequote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3982\u001b[0m \u001b[43m    \u001b[49m\u001b[43mescapechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mescapechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3983\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3984\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/formats/format.py:1014\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[0;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[1;32m    993\u001b[0m     created_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    995\u001b[0m csv_formatter \u001b[38;5;241m=\u001b[39m CSVFormatter(\n\u001b[1;32m    996\u001b[0m     path_or_buf\u001b[38;5;241m=\u001b[39mpath_or_buf,\n\u001b[1;32m    997\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     formatter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt,\n\u001b[1;32m   1013\u001b[0m )\n\u001b[0;32m-> 1014\u001b[0m \u001b[43mcsv_formatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[1;32m   1017\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/formats/csvs.py:251\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;124;03mCreate the writer & save.\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;241m=\u001b[39m csvlib\u001b[38;5;241m.\u001b[39mwriter(\n\u001b[1;32m    261\u001b[0m         handles\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[1;32m    262\u001b[0m         lineterminator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlineterminator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    267\u001b[0m         quotechar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquotechar,\n\u001b[1;32m    268\u001b[0m     )\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 63] File name too long: 'dataOpenAQ_los-angeles_pm25_2016-2024_2775-25551-15731-25196-24000-1654141-1654143-1654156-1654168-1654191-2000869-1654217-2000834-1654333-1654360-2000475-2000731-2000674-1654499-1654545-1654577-1654556-1654629-2000900-1999896-2001073-2001289-2001343-2088604-2088548-2088589-2000821-2000905-2000768-2000525-2000753-2000561-2000676-2001076-2000935-2000945-2000750-2001039-2000499-2000798-2000553-2000565-2000718-2000977-2000505-2000853-2000522-2000723-2000819-2000729-2001006-2000981-2000667-2000840-2000567-2000963-2000537-2000471-2001038-2000956-2000694-2000953-2000629-2000671-2000967-2000999-2000660-2001013-2000613-2000483-2000550-2000832-2000796-2000934-2001040-2000838-2001062-2000933-2000443-2000993-2000942-2000852-2000445-2001033-2000984.csv'"
     ]
    }
   ],
   "source": [
    "# GET OPENAQ AIR QUALITY DAILY AVERAGES DATA...\n",
    "print(AQbyWeather.selectedScenario.getSummary())\n",
    "aq_df = AQbyWeather.getOpenAqDataFrame() # Gets nearby Location IDs THEN gets associated daily averages.\n",
    "print(aq_df)\n",
    "\n",
    "if len(aq_df) > 0:\n",
    "    # Output DataFrame properties...\n",
    "    print('aq_df.shape =', aq_df.shape)\n",
    "    display(aq_df)\n",
    "    aq_df.to_csv(AQbyWeather.getFilenameOpenAQ(), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the NOAA GSOD weather data with our OpenAQ data by DATE...\n",
    "# Perform another column drop to remove columns we don't want as features/inputs.\n",
    "# This column removal will NOT be necessary once we can use Autogluon ignore_columns param (TBD).\n",
    "print(AQbyWeather.selectedScenario.getSummary())\n",
    "# Debug merge operation\n",
    "print(\"NOAA GSOD data shape:\", noaagsod_df.shape)\n",
    "print(\"\\nNOAA GSOD sample dates:\")\n",
    "print(noaagsod_df['DATE'].head())\n",
    "print(noaagsod_df.columns.tolist())   # list of all column names\n",
    "\n",
    "print(\"\\nOpenAQ data shape:\", aq_df.shape) \n",
    "print(\"\\nOpenAQ sample dates:\")\n",
    "print(aq_df['day'].head())\n",
    "\n",
    "# Check for date format consistency\n",
    "print(\"\\nNOAA date type:\", noaagsod_df['DATE'].dtype)\n",
    "print(\"OpenAQ date type:\", aq_df['day'].dtype)\n",
    "\n",
    "# Attempt merge with debug info\n",
    "merged_df = AQbyWeather.getMergedDataFrame(noaagsod_df, aq_df)\n",
    "print(\"\\nMerged data shape:\", merged_df.shape)\n",
    "if(len(merged_df) > 0):\n",
    "    # Output DataFrame properties...\n",
    "    print('merged_df.shape =', merged_df.shape)\n",
    "    display(merged_df)\n",
    "    merged_df.groupby([AQbyWeather.mlTargetLabel]).size().plot(kind=\"bar\")\n",
    "    merged_df.to_csv(AQbyWeather.getFilenameOther(\"dataMERGED\"), index=False)\n",
    "\n",
    "display(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogluon.features.generators import PipelineFeatureGenerator, CategoryFeatureGenerator, IdentityFeatureGenerator\n",
    "from autogluon.common.features.types import R_INT, R_FLOAT\n",
    "from autogluon.features.generators import AutoMLPipelineFeatureGenerator\n",
    "\n",
    "columns_to_drop=['MIN_ATTRIBUTES', 'MAX_ATTRIBUTES']\n",
    "merged_df = merged_df.drop(columns=[col for col in columns_to_drop if col in merged_df.columns])\n",
    "\n",
    "merged_df = merged_df.replace(r'^\\s*$', np.nan, regex=True)\n",
    "\n",
    "auto_ml_pipeline_feature_generator = AutoMLPipelineFeatureGenerator()\n",
    "merged_df = auto_ml_pipeline_feature_generator.fit_transform(X=merged_df)\n",
    "\n",
    "print(\"\\nColumn dtypes:\")\n",
    "print(merged_df.dtypes)\n",
    "\n",
    "# mypipeline = PipelineFeatureGenerator(\n",
    "#     generators = [[        \n",
    "#         CategoryFeatureGenerator(maximum_num_cat=10),  # Overridden from default.\n",
    "#         IdentityFeatureGenerator(infer_features_in_args=dict(valid_raw_types=[R_INT, R_FLOAT])),\n",
    "#     ]]\n",
    "# )\n",
    "# mypipeline.fit_transform(X=merged_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize correlations in our merged dataframe...\n",
    "print(AQbyWeather.selectedScenario.getSummary())\n",
    "correlations = merged_df.corr()\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(correlations, vmin=-1, vmax=1)\n",
    "fig.colorbar(cax)\n",
    "ticks = np.arange(0, len(merged_df.columns), 1)\n",
    "ax.set_xticks(ticks)\n",
    "ax.set_yticks(ticks)\n",
    "ax.set_xticklabels(merged_df.columns)\n",
    "ax.set_yticklabels(merged_df.columns)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the NOAA GSOD weather data with our OpenAQ data by DATE...\n",
    "# Perform another column drop to remove columns we don't want as features/inputs.\n",
    "# This column removal will NOT be necessary once we can use Autogluon ignore_columns param (TBD).\n",
    "print(AQbyWeather.selectedScenario.getSummary())\n",
    "# Debug merge operation\n",
    "print(\"NOAA GSOD data shape:\", noaagsod_df.shape)\n",
    "print(\"\\nNOAA GSOD sample dates:\")\n",
    "print(noaagsod_df['DATE'].head())\n",
    "\n",
    "print(\"\\nOpenAQ data shape:\", aq_df.shape) \n",
    "print(\"\\nOpenAQ sample dates:\")\n",
    "print(aq_df['day'].head())\n",
    "\n",
    "# Check for date format consistency\n",
    "print(\"\\nNOAA date type:\", noaagsod_df['DATE'].dtype)\n",
    "print(\"OpenAQ date type:\", aq_df['day'].dtype)\n",
    "\n",
    "# Attempt merge with debug info\n",
    "merged_df = AQbyWeather.getMergedDataFrame(noaagsod_df, aq_df)\n",
    "print(\"\\nMerged data shape:\", merged_df.shape)\n",
    "if(len(merged_df) > 0):\n",
    "    merged_df.groupby([AQbyWeather.mlTargetLabel]).size().plot(kind=\"bar\")\n",
    "    merged_df.to_csv(AQbyWeather.getFilenameOther(\"dataMERGED\"), index=False)\n",
    "\n",
    "display(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional import statements for autogluon+sklearn and split out train_df + validate_df data...\n",
    "print(AQbyWeather.selectedScenario.getSummary())\n",
    "from autogluon.tabular import TabularPredictor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "train_df, validate_df = train_test_split(merged_df, test_size=0.25, random_state=1)\n",
    "print('Number of training samples:', len(train_df))\n",
    "print('Number of validation samples:', len(validate_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the test_df data and remove the target label column...\n",
    "print(AQbyWeather.selectedScenario.getSummary())\n",
    "test_df=validate_df.drop([AQbyWeather.mlTargetLabel], axis=1)\n",
    "display(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogluon.common import space\n",
    "\n",
    "nn_options = {  # specifies non-default hyperparameter values for neural network models\n",
    "    'num_epochs': 50,  # number of training epochs (controls training time of NN models)\n",
    "    'learning_rate': space.Real(1e-4, 1e-2, default=1e-3, log=True),  # learning rate used in training (real-valued hyperparameter searched on log-scale)\n",
    "    'activation': space.Categorical('relu', 'softrelu', 'tanh'),  # activation function used in NN (categorical hyperparameter, default = first entry)\n",
    "    'dropout_prob': space.Real(0.0, 0.5, default=0.1),  # dropout probability (real-valued hyperparameter)\n",
    "}\n",
    "\n",
    "gbm_options = {  # specifies non-default hyperparameter values for lightGBM gradient boosted trees\n",
    "    'num_boost_round': 100,  # number of boosting rounds (controls training time of GBM models)\n",
    "    'num_leaves': space.Int(lower=26, upper=66, default=36),  # number of leaves in trees (integer hyperparameter)\n",
    "}\n",
    "\n",
    "hyperparameters = {  # hyperparameters of each model type\n",
    "                   'GBM': gbm_options,\n",
    "                   'NN_TORCH': nn_options,  # NOTE: comment this line out if you get errors on Mac OSX\n",
    "                  }  # When these keys are missing from hyperparameters dict, no models of that type are trained\n",
    "\n",
    "num_trials = 10  # try at most 5 different hyperparameter configurations for each type of model\n",
    "search_strategy = 'auto'  # to tune hyperparameters using random search routine with a local scheduler\n",
    "time_limit = 60 * 60\n",
    "hyperparameter_tune_kwargs = {  # HPO is not performed unless hyperparameter_tune_kwargs is specified\n",
    "    'num_trials': num_trials,\n",
    "    'scheduler' : 'local',\n",
    "    'searcher': search_strategy,\n",
    "}  # Refer to TabularPredictor.fit docstring for all valid values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use AutoGluon TabularPredictor to fit a model for our training data...\n",
    "display(AQbyWeather.selectedScenario.getSummary()) #Using display for consistent/sequential output order.\n",
    "predictor = TabularPredictor(label=AQbyWeather.mlTargetLabel, \n",
    "                             eval_metric=AQbyWeather.mlEvalMetric, \n",
    "                             path=AQbyWeather.selectedScenario.getModelPath())\n",
    "predictor.fit(train_data=train_df, auto_stack=True, time_limit=time_limit, hyperparameters=hyperparameters,\n",
    "    hyperparameter_tune_kwargs=hyperparameter_tune_kwargs, verbosity=3, presets='best_quality')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dataframes for feature importance + model leaderboard AND get+display model evaluation...\n",
    "display(AQbyWeather.selectedScenario.getSummary()) #Using display for consistent/sequential output order.\n",
    "featureimp_df   = predictor.feature_importance(validate_df)\n",
    "leaderboard_df  = predictor.leaderboard(validate_df, silent=True)\n",
    "modelEvaluation = predictor.evaluate(validate_df, auxiliary_metrics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View Autogluon Individual Model Leaderboard...\n",
    "print(AQbyWeather.selectedScenario.getSummary())\n",
    "display(leaderboard_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View and Plot Feature Importance... (this various from Scenario to Scenario)\n",
    "print(AQbyWeather.selectedScenario.getSummary())\n",
    "display(featureimp_df)\n",
    "featureimp_df.drop(columns=[\"n\"]).plot(kind=\"bar\", figsize=(12, 4), xlabel=\"feature\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load + Use Our Model (this line is unnecessary, but shows how to load a built model)...\n",
    "predictor = TabularPredictor.load(AQbyWeather.selectedScenario.getModelPath())\n",
    "\n",
    "# Make Predictions, which are saved to an array: y_pred\n",
    "print(AQbyWeather.selectedScenario.getSummary())\n",
    "y_pred = predictor.predict(test_df)\n",
    "display(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get true label values as an array: y_true\n",
    "print(AQbyWeather.selectedScenario.getSummary())\n",
    "y_true = validate_df[AQbyWeather.mlTargetLabel]\n",
    "display(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Confusion Matrix (CM), Get Additional CM Data, View CM...\n",
    "# Learn More: https://towardsdatascience.com/confusion-matrix-what-is-it-e859e1bbecdc\n",
    "#             https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html\n",
    "print(AQbyWeather.selectedScenario.getSummary())\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Print Confusion Matrix data...\n",
    "cmData = AQbyWeather.getConfusionMatrixData(cm)\n",
    "print(cmData.TN_Output)\n",
    "print(cmData.TP_Output)\n",
    "print(cmData.FN_Output)\n",
    "print(cmData.FP_Output)\n",
    "\n",
    "# Plot Confusion Matrix...\n",
    "cmd = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "cmd.plot(ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and save final results...\n",
    "print(AQbyWeather.selectedScenario.getSummary())\n",
    "resultsFile = AQbyWeather.getFilenameOther(\"dataRESULTS\")\n",
    "results_df = pd.DataFrame()\n",
    "results_df['PREDICTION'] = pd.DataFrame(y_pred)\n",
    "results_df = pd.concat([validate_df, results_df], axis=1)\n",
    "results_df.to_csv(resultsFile, index=False)\n",
    "print(f\"Results saved to {resultsFile}. DONE.\")\n",
    "display(results_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
